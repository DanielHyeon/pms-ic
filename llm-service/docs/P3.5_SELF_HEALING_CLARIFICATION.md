# P3.5: Self-Healing Fallback via Clarifying Questions (B-Plan v2.1)

**Priority**: High (Empty-data UX breakthrough)
**Timeline**: 1-2 days (post-P3)
**Goal**: Transform "no data" dead-ends into actionable clarifying questions that recover in a single turn
**Approach**: B-Plan (4 intents simultaneous rollout with safety contracts)

---

## 0. P3.5 설계의 본질 (왜 이게 UX 돌파구인가)

### "없습니다"를 "선택 → 즉시 재실행"으로 바꾼 게 핵심

P1의 graceful degradation이 "대안 안내" 수준이었다면, P3.5는 **사용자 선택이 곧 시스템 상태(context)로 반영**되어, 다음 턴이 아니라 **'즉시 재실행'**으로 정상 응답을 보장합니다.

> **되묻기 자체가 아니라, 되묻기의 결과가 시스템 상태에 고정되는 계약(Contract)이 본질입니다.**

### Classifier Bypass가 회귀를 막는 '필수 안전장치'

이 설계에서 가장 중요한 문장:

> **pending이 있으면 classifier를 돌리지 않는다.**

이게 없으면 사용자가 "1"을 입력했을 때 classifier가 이를 일반 질의로 재해석하면서 **STATUS_METRIC/UNKNOWN으로 빠지는 회귀**가 다시 살아납니다.

### "자연어 이해" 최소화가 운영적으로 정답

되묻기 답변을 LLM로 재해석하지 않고 **숫자/키/alias 매칭**으로 처리한 건 장애/회귀/예측불가를 줄이는 데 크게 기여합니다.

---

## 1. P3.5의 정체성

> **P3.5는 "없습니다"를 "어떻게 도와드릴까요?"로 바꾸는 단계다.**

| Phase | 역할 |
|-------|------|
| P0 | Intent 라우팅 정상화 (📊 블랙홀 제거) |
| P1 | 데이터 쿼리 안전장치 (graceful degradation) |
| P2 | 품질 개선 (threshold tuning, retry logic) |
| P3 | 설명 책임 + 자가 복구 루프 |
| **P3.5** | **되묻기 표준화 + 1턴 복구 UX (B-Plan)** |

P3가 "데이터 없음 → RecoveryPlan 제안"을 했다면, P3.5는 그 RecoveryPlan 중 **ask_clarification**을 완전히 표준화하여:
- 사용자가 **선택지 중 하나를 고르면**
- 시스템이 **즉시 재실행하여 정상 응답**으로 복구되는 것을 보장합니다.

### B-Plan이란?

**B-Plan = 4개 intent(Sprint/Backlog/Risk/TaskDue) 동시 적용**

- 장점: 한 번에 끝남
- 리스크: 변경 폭이 커서 P2의 안전장치(회귀 방지/라우팅 안정성)를 깨먹을 수 있음
- 해결책: **표준 계약(Contract) + 상태 머신 + 핸들러 패턴 + 렌더러 + 테스트 + 메트릭**을 완성형으로 제공

---

## 2. B-Plan 필수 성공 조건 (회귀/블랙홀 방지)

| # | 조건 | 근거 |
|---|------|------|
| 1 | 되묻기는 항상 **선택지(2~6개)** + **번호 입력 지원** | 텍스트만 요구 시 회복률 급락 |
| 2 | 사용자 답변은 **자연어 해석이 아닌 옵션 매칭**(번호/키워드) 중심 | LLM 재해석 시 STATUS_METRIC 회귀 위험 |
| 3 | pending_clarification은 **TTL(10분) + 한 세션에 1개만** | 오래된 질문 혼재 방지 |
| 4 | 사용자 선택 → **context patch 적용 후 같은 intent 재실행** | 명시적 override는 허용 |
| 5 | **Clarification 처리 경로는 classifier 우회** | STATUS_METRIC 블랙홀 방지 (가장 중요!) |
| 6 | 옵션 매칭 실패 시 **같은 질문 재표시** (LLM 재해석 X) | "1번 누르라는 거구나" 즉시 인지 |

---

## 3. P3.5 핵심 원칙 5가지

### 원칙 1: 되묻기는 "한 번에 하나의 축만"

한 질문에 scope/time/entity를 다 물으면 사용자는 포기합니다.

| Good | Bad |
|------|-----|
| "스프린트 기준으로 볼까요, 프로젝트 전체로 볼까요?" | "스프린트/기간/담당자/파트 중 뭘로 볼까요?" |

### 원칙 2: 질문은 반드시 "선택지"를 포함

- 텍스트 입력만 요구하면 회복률이 **급락**
- 선택지 **2~4개**가 최적
- "기타(직접 입력)"은 **마지막 옵션**

### 원칙 3: 되묻기는 "내부 상태를 바꾸는 계약"

사용자가 선택하면 다음 턴에서:
1. **scope가 확정**되고
2. **쿼리가 바뀌**고
3. **정상 응답이 나와야** 합니다

### 원칙 4: 질문은 "추정 기본값 + 사용자 확인"

아무것도 모르면 가장 가능성 높은 default를 제시하고 확인 받습니다.

```
❓ 현재 ACTIVE 스프린트가 없어요.
1) 최근 완료 스프린트 기준 (추천) ← default
2) 특정 스프린트 선택
3) 프로젝트 전체 진행률
```

### 원칙 5: 되묻기는 "반드시 1턴 안에 해결"을 목표로

2~3턴으로 늘어나면 "대화형 시스템"이 아니라 "귀찮은 시스템"이 됩니다.

---

## 4. P3.5 Trigger 정의

P3.5는 아래 **3가지 상황에서만** 발동됩니다:

| Trigger | 정의 | 예시 |
|---------|------|------|
| **Empty** | 쿼리 성공, 결과 0건 | 백로그 0, 위험 0, 태스크 0 |
| **Missing Scope** | 필요한 scope가 없어서 조회 불가 | active sprint 없음 |
| **Ambiguous Query** | 의도는 맞지만 범위가 모호 | "진행률" (스프린트? 프로젝트?) |

**※ Query Failure (DB 장애)는 P3.5가 아니라 P3의 "Unavailable Recovery"로 처리**

---

## 5. P3.5 Risk Mitigations

### 기본 리스크 (v2.0)

| Risk | Issue | Mitigation |
|------|-------|------------|
| **(A)** | 되묻기만 하고 복구 안됨 | **State Machine으로 다음 턴 재실행 강제** |
| **(B)** | 선택지 없이 텍스트만 요구 | **ClarificationOption 필수화** |
| **(C)** | 다중 축 질문으로 혼란 | **single-axis rule 검증** |
| **(D)** | 옛날 질문이 섞임 | **TTL(600초) 적용** |
| **(E)** | 되묻기 과다 → UX 저하 | **clarification_trigger_rate 임계값** |
| **(F)** | 복구 실패 측정 불가 | **clarification_resolution_rate 메트릭** |
| **(G)** | STATUS_METRIC 블랙홀 회귀 | **pending 존재 시 classifier 우회** |

### v2.1 운영 리스크 8가지 (실제 운영에서 터지는 지점들)

| Risk | Issue | Mitigation |
|------|-------|------------|
| **(R1)** | "single-axis rule"이 코드로 강제되지 않음 - factory가 늘어나면 질문이 복합축이 됨 | **ClarificationQuestion에 axis 필드 추가 + validation** |
| **(R2)** | alias contains match 오매칭 위험 - 짧은 단어가 의도치 않게 매칭 | **완전일치 우선 + 단어 경계 정규식으로 강화** |
| **(R3)** | "옵션 선택 후 재실행"이 또 실패할 때 UX 빈약 | **2차 fallback 내장: 1차 실패 시 자동 대안 전환** |
| **(R4)** | context patch의 "수명"이 정의되지 않음 - 다음 unrelated 질문에도 남음 | **ephemeral_context 분리: 복구 후 자동 삭제** |
| **(R5)** | pending 1개 정책에서 "질문 전환" 처리 미정의 - 다른 주제 질문 시 같은 clarification 반복 | **abandon detection 규칙: 토픽 변경 시 pending 폐기** |
| **(R6)** | metrics "분모/분자 정의"가 불명확 | **레이블에 reason 추가: abandoned{ttl_expired, topic_change}, resolved{numeric, alias, key}** |
| **(R7)** | has_data 정의가 intent마다 다르면 혼란 | **일관된 기준 명시: "다음 액션 가능한 정보" = has_data=True** |
| **(R8)** | 질문 본문에 "왜 물어보는지"가 빠지면 맥락 상실 | **message에 이유 1문장 강제: "현재 ACTIVE 스프린트가 없어요."** |

---

## 6. Implementation Checklist

| # | Checklist Item | Verification |
|---|----------------|--------------|
| 1 | **Empty data triggers clarification** | Test: empty sprint → clarification.question_id exists |
| 2 | **Clarification has 2-4 options** | Test: `2 <= len(clarification.options) <= 4` |
| 3 | **At least one option is default** | Test: `any(opt.is_default for opt in options)` |
| 4 | **User selection triggers re-execution** | Test: select option → new response with data |
| 5 | **Pending clarification has TTL** | Test: after 600s → pending cleared |
| 6 | **1-turn resolution rate tracked** | Metric: `clarification_resolution_rate >= 0.7` |
| 7 | **Renderer displays numbered options** | Visual: "1) 2) 3)" format |
| 8 | **Free-text only as last option** | Test: `allow_free_text` only with explicit flag |
| 9 | **Classifier bypassed for pending** | Test: pending exists → skip classification |
| 10 | **has_data distinguishes empty vs failure** | Test: success=True, has_data=False for empty |

---

## 7. Files to Modify/Create

| File | Change Type | Description |
|------|-------------|-------------|
| `clarification.py` | **NEW** | Clarification 데이터 모델 (aliases 지원) |
| `clarification_factory.py` | **NEW** | 의도별 표준 질문 팩토리 |
| `response_contract.py` | **MODIFY** | clarification + has_data 필드 추가 |
| `chat_workflow_v2.py` | **MODIFY** | **상태 머신 (가장 중요)**: pending 우선 처리, classifier 우회 |
| `intent_handlers.py` | **MODIFY** | 4개 intent에 clarification 동시 적용 |
| `response_renderer.py` | **MODIFY** | 질문/선택지 표준 출력 |
| `metrics.py` | **MODIFY** | P3.5 메트릭 추가 |
| `tests/test_clarification_flow_regression.py` | **NEW** | **복구 성공** 테스트 (생성만 아님) |

---

## 8. Implementation Details

### 8.1 clarification.py (NEW FILE)

**Location**: `/llm-service/clarification.py`

```python
"""
P3.5: Self-Healing Fallback via Clarifying Questions.

PURPOSE:
- Transform "no data" into actionable questions with options
- Enable 1-turn recovery by applying user selection to context
- Maintain conversation flow without dead-ends

B-PLAN ENHANCEMENTS:
- aliases field for flexible input matching
- Numeric + keyword + alias matching
- TTL-based expiration
- Single pending per session

============================================================
TRIGGER CONDITIONS
============================================================
- "empty": Query succeeded but returned 0 rows
- "missing_scope": Required scope (e.g., active sprint) not found
- "ambiguous": Intent matched but range unclear
============================================================
"""

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

@dataclass
class ClarificationOption:
    """
    A single selectable option in a clarifying question.

    B-PLAN ADDITION:
    - aliases: Alternative match phrases for non-numeric input
    """
    key: str                   # stable id: "scope:last_completed_sprint"
    label: str                 # user-facing label
    value: Dict[str, Any]      # context patch to apply
    is_default: bool = False
    # B-PLAN: match phrases for non-numeric input
    aliases: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "key": self.key,
            "label": self.label,
            "value": self.value,
            "is_default": self.is_default,
            "aliases": self.aliases,
        }


@dataclass
class ClarificationQuestion:
    """
    A clarifying question with options for user selection.

    CONSTRAINTS (enforced by validation):
    - 2-4 options required
    - At least one option must be default
    - Single-axis focus (don't ask multiple things at once)

    v2.1 ADDITIONS:
    - axis field for single-axis enforcement (R1)
    - message must include reason (R8)
    """
    question_id: str           # stable id for tests/metrics
    intent: str                # original intent
    message: str               # user-facing question (MUST include reason)
    options: List[ClarificationOption]
    allow_free_text: bool = False
    free_text_hint: Optional[str] = None
    ttl_seconds: int = 600     # 10 minutes
    # v2.1: Single-axis enforcement (R1)
    axis: str = "scope"        # "scope" | "time" | "entity" | "metric" | "action"

    # Valid axis values
    VALID_AXES = {"scope", "time", "entity", "metric", "action"}

    def __post_init__(self):
        """Validate clarification question constraints."""
        self._validate()

    def _validate(self) -> None:
        """
        Enforce P3.5 constraints.

        Raises:
            ValueError: If constraints are violated
        """
        # Constraint 1: 2-4 options required
        if not (2 <= len(self.options) <= 4):
            raise ValueError(
                f"ClarificationQuestion must have 2-4 options, got {len(self.options)}"
            )

        # Constraint 2: At least one default
        if not any(opt.is_default for opt in self.options):
            raise ValueError(
                "ClarificationQuestion must have at least one default option"
            )

        # Constraint 3: question_id must be present
        if not self.question_id:
            raise ValueError("ClarificationQuestion must have a question_id")

        # v2.1 Constraint 4: axis must be valid (R1)
        if self.axis not in self.VALID_AXES:
            raise ValueError(
                f"ClarificationQuestion axis must be one of {self.VALID_AXES}, got '{self.axis}'"
            )

        # v2.1 Constraint 5: message must include context/reason (R8)
        # Weak check: message should be long enough to include reason
        if len(self.message) < 15:
            raise ValueError(
                "ClarificationQuestion message too short - must include reason/context"
            )

    def match_user_input(self, user_input: str) -> Optional[ClarificationOption]:
        """
        Match user input to an option.

        v2.1 ENHANCED MATCHING ORDER (R2 - 오매칭 방지):
        1. Numeric input ("1", "2", "3") - highest priority
        2. Key exact match ("scope:sprint")
        3. Alias exact match (전체 단어 일치)
        4. Alias word boundary match (단어 경계 포함)

        Returns:
            Matched option or None
        """
        import re
        t = (user_input or "").strip().lower()

        # 1) Numeric - highest priority
        if t.isdigit():
            idx = int(t) - 1
            if 0 <= idx < len(self.options):
                return self.options[idx]

        # 2) Key exact match
        for opt in self.options:
            if t == opt.key.lower():
                return opt

        # 3) Alias exact match (v2.1 - 완전일치 우선)
        for opt in self.options:
            for alias in opt.aliases:
                if t == alias.lower():
                    return opt

        # 4) Alias word boundary match (v2.1 - 단어 경계 포함)
        # "전체"가 "전체적으로"에 잘못 매칭되지 않도록
        for opt in self.options:
            for alias in opt.aliases:
                # Word boundary pattern: \b{alias}\b (한글은 \s로 대체)
                pattern = r'(?:^|[\s,.]|$)' + re.escape(alias.lower()) + r'(?:[\s,.]|$)'
                if re.search(pattern, t):
                    return opt

        return None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "question_id": self.question_id,
            "intent": self.intent,
            "message": self.message,
            "options": [opt.to_dict() for opt in self.options],
            "allow_free_text": self.allow_free_text,
            "free_text_hint": self.free_text_hint,
            "ttl_seconds": self.ttl_seconds,
        }


@dataclass
class PendingClarification:
    """
    Stored in session state.
    Minimal data to resolve the next user turn.
    """
    question: ClarificationQuestion
    created_at_epoch_ms: int
    ttl_seconds: int = 600

    def to_dict(self) -> Dict[str, Any]:
        return {
            "question": self.question.to_dict(),
            "created_at_epoch_ms": self.created_at_epoch_ms,
            "ttl_seconds": self.ttl_seconds,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "PendingClarification":
        """Reconstruct from dictionary (for session storage)."""
        q_data = data["question"]
        options = [
            ClarificationOption(
                key=opt["key"],
                label=opt["label"],
                value=opt["value"],
                is_default=opt.get("is_default", False),
                aliases=opt.get("aliases", []),
            )
            for opt in q_data["options"]
        ]
        # Create question without validation (already validated)
        question = object.__new__(ClarificationQuestion)
        question.question_id = q_data["question_id"]
        question.intent = q_data["intent"]
        question.message = q_data["message"]
        question.options = options
        question.allow_free_text = q_data.get("allow_free_text", False)
        question.free_text_hint = q_data.get("free_text_hint")
        question.ttl_seconds = q_data.get("ttl_seconds", 600)

        return cls(
            question=question,
            created_at_epoch_ms=data["created_at_epoch_ms"],
            ttl_seconds=data.get("ttl_seconds", 600),
        )
```

---

### 8.2 response_contract.py (MODIFY)

**Location**: `/llm-service/response_contract.py`

**Key Changes**:
- Add `has_data: bool` field to distinguish empty vs failure
- Add `clarification: Optional[ClarificationQuestion]` field

```python
# response_contract.py (B-PLAN ENHANCEMENT)
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from clarification import ClarificationQuestion

@dataclass
class ResponseContract:
    """
    Standard response contract for all intent handlers.

    B-PLAN ADDITIONS:
    - has_data: bool - distinguishes empty data (success=True, has_data=False)
                       from query failure (success=False)
    - clarification: Optional ClarificationQuestion for recovery
    """
    intent: str
    reference_time: str
    scope: Dict[str, Any]
    data: Dict[str, Any]

    success: bool = True              # query success (not failure)
    has_data: bool = True             # meaningful data exists (not empty)
    warnings: Optional[List[str]] = None
    tips: Optional[List[str]] = None
    provenance: Optional[str] = None

    # P3.5 B-PLAN
    clarification: Optional[ClarificationQuestion] = None

    def has_clarification(self) -> bool:
        """Check if this response has a clarification question."""
        return self.clarification is not None

    def is_empty_response(self) -> bool:
        """Check if this is an empty but successful response."""
        return self.success and not self.has_data

    def is_failure(self) -> bool:
        """Check if this is a query failure."""
        return not self.success

    def to_dict(self) -> Dict[str, Any]:
        result = {
            "intent": self.intent,
            "reference_time": self.reference_time,
            "scope": self.scope,
            "data": self.data,
            "success": self.success,
            "has_data": self.has_data,
            "warnings": self.warnings,
            "tips": self.tips,
            "provenance": self.provenance,
        }
        if self.clarification:
            result["clarification"] = self.clarification.to_dict()
        return result
```

---

### 8.3 chat_workflow_v2.py (MODIFY) - **가장 중요**

**Location**: `/llm-service/chat_workflow_v2.py`

**B-Plan 핵심**: pending이 있으면 **classifier를 돌리지 말고 먼저 해결 시도**
(돌리면 STATUS_METRIC 같은 generic intent로 떨어지는 회귀가 다시 생길 수 있음)

```python
"""
chat_workflow_v2.py - B-PLAN STATE MACHINE (v2.1)

B-PLAN CRITICAL RULES:
1. If pending_clarification exists, DO NOT run classifier.
2. v2.1: Detect topic change and abandon pending if user changed subject.
3. v2.1: Use ephemeral_context for recovery patches.

This prevents STATUS_METRIC blackhole regression.
"""

import time
import re
from typing import Dict, Any, Optional
from clarification import PendingClarification, ClarificationQuestion
from response_contract import ResponseContract
from metrics import record_clarification_abandoned, record_clarification_resolved

def _now_ms() -> int:
    return int(time.time() * 1000)

def _is_expired(pending: PendingClarification) -> bool:
    return (_now_ms() - pending.created_at_epoch_ms) > (pending.ttl_seconds * 1000)


# =============================================================================
# v2.1: Topic Change Detection (R5 - abandon detection)
# =============================================================================

ABANDON_KEYWORDS = {"취소", "그만", "다음에", "다른 질문", "무시", "넘어가", "skip", "cancel"}

def _is_topic_change(user_text: str, pending: PendingClarification) -> bool:
    """
    Detect if user is trying to change topic instead of answering clarification.

    v2.1 RULES (R5):
    1. Explicit abandon keywords -> True
    2. Long sentence (>20 chars) + no option match -> likely topic change
    3. Question mark in input -> likely new question

    Returns:
        True if topic change detected (pending should be abandoned)
    """
    t = (user_text or "").strip().lower()

    # Rule 1: Explicit abandon keywords
    for keyword in ABANDON_KEYWORDS:
        if keyword in t:
            return True

    # Rule 2: Long sentence without option match
    if len(t) > 20:
        opt = pending.question.match_user_input(user_text)
        if opt is None:
            return True

    # Rule 3: Question mark suggests new question
    if "?" in t or "？" in t:
        return True

    return False


def _match_option(user_text: str, question: ClarificationQuestion):
    """
    Match numeric input '1', '2' or exact key, or alias phrases.

    v2.1 MATCHING ORDER (R2 - 오매칭 방지):
    1. Numeric (highest priority)
    2. Key exact
    3. Alias exact
    4. Alias word boundary
    """
    # Delegate to ClarificationQuestion's enhanced matching
    return question.match_user_input(user_text)


def handle_message(session, user_message: str) -> str:
    """
    High-level flow:

    B-PLAN STATE MACHINE (v2.1):
    1) If pending clarification exists -> check topic change first
    2) If not topic change -> resolve clarification (skip classifier!)
    3) Else classify intent -> handler -> contract
    4) If contract has clarification -> store it
    5) Clear ephemeral_context after handler execution
    6) Render response

    CRITICAL: Step 2 bypasses classifier to prevent STATUS_METRIC blackhole.
    """

    # =========================================================================
    # 1) RESOLVE PENDING CLARIFICATION FIRST (CLASSIFIER BYPASS)
    # =========================================================================
    pending_data = session.state.get("pending_clarification")
    if pending_data:
        pending_obj = PendingClarification.from_dict(pending_data)
        question_id = pending_obj.question.question_id

        # Case 1: TTL expired
        if _is_expired(pending_obj):
            session.state.pop("pending_clarification", None)
            session.save_state()
            record_clarification_abandoned(question_id)
            # Continue to normal classification

        # v2.1 Case 2: Topic change detected (R5)
        elif _is_topic_change(user_message, pending_obj):
            session.state.pop("pending_clarification", None)
            _clear_ephemeral_context(session)  # v2.1: Clear recovery patches
            session.save_state()
            record_clarification_abandoned(question_id)
            # Continue to normal classification

        # Case 3: Try to resolve
        else:
            opt = _match_option(user_message, pending_obj.question)

            if opt:
                # v2.1: Apply to ephemeral_context (R4)
                session = _apply_ephemeral_patch(session, opt.value)

                # Re-run original intent (or override)
                intent = pending_obj.question.intent
                if opt.value.get("intent_override"):
                    intent = opt.value["intent_override"]

                # CRITICAL: Clear pending BEFORE executing to avoid loops
                session.state.pop("pending_clarification", None)
                session.save_state()

                ctx = _build_handler_context(session, user_message)
                handler = get_handler(intent)
                contract = handler(ctx)

                # v2.1: Clear ephemeral context after execution (R4)
                _clear_ephemeral_context(session)

                # Record resolution
                record_clarification_resolved(question_id, "matched", turns=1)

                # v2.1: If handler again returns clarification (R3 - 2차 fallback)
                if contract.has_clarification():
                    # Already tried once and still empty -> use secondary fallback
                    contract = _apply_secondary_fallback(contract, session)

                _maybe_store_clarification(session, contract)
                return render(contract)

            # Unmatched input: re-render the same clarification (gentle)
            # DO NOT use LLM to re-interpret - just show the question again
            return render(_clarification_only_contract(pending_obj.question, session))

    # =========================================================================
    # 2) NORMAL CLASSIFICATION ROUTE
    # =========================================================================
    classified_intent = _classify_intent(user_message)  # existing
    ctx = _build_handler_context(session, user_message)
    handler = get_handler(classified_intent)
    contract = handler(ctx)

    # =========================================================================
    # 3) STORE CLARIFICATION IF PRESENT
    # =========================================================================
    _maybe_store_clarification(session, contract)

    # =========================================================================
    # 4) RENDER
    # =========================================================================
    return render(contract)


def _maybe_store_clarification(session, contract: ResponseContract):
    """Store pending clarification if contract has one."""
    if contract.has_clarification():
        session.state["pending_clarification"] = {
            "question": contract.clarification.to_dict(),
            "created_at_epoch_ms": _now_ms(),
            "ttl_seconds": contract.clarification.ttl_seconds
        }
        session.save_state()
        # Record metric
        from metrics import record_clarification_triggered
        record_clarification_triggered(
            contract.intent,
            contract.clarification.question_id
        )


# =============================================================================
# v2.1: Ephemeral Context (R4 - context patch 수명 관리)
# =============================================================================

def _apply_ephemeral_patch(session, patch: dict):
    """
    v2.1: Apply EPHEMERAL changes for recovery (R4).

    These are cleared after handler execution to prevent
    "선택의 잔상"이 다음 unrelated 질문에 남는 문제.

    Examples:
    - fallback_mode
    - time_window_days
    - scope overrides
    """
    session.state.setdefault("ephemeral_context", {})
    session.state["ephemeral_context"].update(patch)
    session.save_state()
    return session


def _clear_ephemeral_context(session):
    """
    v2.1: Clear ephemeral context after handler execution (R4).

    Called after:
    - Successful recovery
    - Topic change abandon
    - TTL expiration
    """
    session.state.pop("ephemeral_context", None)
    session.save_state()


def _apply_context_patch(session, patch: dict):
    """
    Apply STICKY changes (explicit user preferences).

    DEPRECATED in v2.1: Use _apply_ephemeral_patch for recovery.
    Keep for backward compatibility with explicit user settings.
    """
    session.state.setdefault("context", {})
    session.state["context"].update(patch)
    session.save_state()
    return session


# =============================================================================
# v2.1: Secondary Fallback (R3 - 2차 fallback)
# =============================================================================

# Intent-specific secondary fallbacks when first recovery fails
SECONDARY_FALLBACKS = {
    "SPRINT_PROGRESS": {
        "fallback_mode": "project_completion",
        "intent_override": "STATUS_METRIC",
        "tip": "완료된 스프린트도 없어 프로젝트 전체 완료율로 표시했어요."
    },
    "BACKLOG_LIST": {
        "fallback_mode": "show_templates",
        "tip": "백로그가 비어있어 샘플 템플릿을 보여드려요."
    },
    "RISK_ANALYSIS": {
        "fallback_mode": "label_policy",
        "tip": "리스크 후보도 없어 리스크 라벨링 정책을 안내해 드려요."
    },
    "TASK_DUE_THIS_WEEK": {
        "fallback_mode": "all_open",
        "tip": "마감 작업이 없어 미완료 전체 작업을 보여드려요."
    },
}


def _apply_secondary_fallback(contract: ResponseContract, session) -> ResponseContract:
    """
    v2.1: Apply secondary fallback when first recovery also fails (R3).

    If user selected option 1 but still got clarification (data still empty),
    auto-switch to a safer fallback that won't fail.

    This prevents "1 눌렀는데 또 질문" UX.
    """
    intent = contract.intent.upper()
    fallback = SECONDARY_FALLBACKS.get(intent)

    if fallback is None:
        return contract  # No secondary fallback defined

    # Apply secondary fallback
    _apply_ephemeral_patch(session, {
        "fallback_mode": fallback.get("fallback_mode"),
        "intent_override": fallback.get("intent_override"),
    })

    # Re-run handler with secondary fallback
    ctx = _build_handler_context(session, "")
    override_intent = fallback.get("intent_override", intent)
    handler = get_handler(override_intent)
    new_contract = handler(ctx)

    # Add tip explaining the auto-switch
    if fallback.get("tip"):
        new_contract.tips = new_contract.tips or []
        new_contract.tips.insert(0, fallback["tip"])

    # Clear clarification to prevent loop
    new_contract.clarification = None

    return new_contract


def _clarification_only_contract(question: ClarificationQuestion, session) -> ResponseContract:
    """Create a contract that only shows the clarification question again."""
    return ResponseContract(
        intent=question.intent,
        reference_time=_get_kst_reference_time(),
        scope={},
        data={},
        success=True,
        has_data=False,
        clarification=question,
        tips=["1~{} 중 번호로 답해 주세요.".format(len(question.options))],
        provenance="Clarification",
    )
```

---

### 8.4 clarification_factory.py (NEW FILE) - B-Plan 공통 유틸

**Location**: `/llm-service/clarification_factory.py`

```python
"""
P3.5 B-PLAN: Clarification Factory

PURPOSE:
- Provide pre-built clarification questions for all 4 intents
- Ensure consistent UX and aliases across scenarios
- Single source of truth for clarification templates

USAGE:
    from clarification_factory import q_sprint_no_active

    if sprint is None:
        return ResponseContract(
            ...
            clarification=q_sprint_no_active(),
        )
"""

from clarification import ClarificationQuestion, ClarificationOption


def q_sprint_no_active(intent: str = "SPRINT_PROGRESS") -> ClarificationQuestion:
    """
    Template: SPRINT_PROGRESS with no active sprint.

    Question ID: sprint.no_active_sprint
    Trigger: Missing Scope (active sprint not found)
    """
    return ClarificationQuestion(
        question_id="sprint.no_active_sprint",
        intent=intent,
        message="현재 ACTIVE 스프린트가 없어요. 어떤 기준으로 진행상황을 볼까요?",
        options=[
            ClarificationOption(
                key="scope:last_completed_sprint",
                label="최근 완료 스프린트 기준 (추천)",
                value={"fallback_mode": "last_completed_sprint"},
                is_default=True,
                aliases=["최근", "완료", "last"]
            ),
            ClarificationOption(
                key="scope:pick_recent_sprint",
                label="최근 스프린트 목록에서 선택",
                value={"fallback_mode": "pick_recent_sprint"},
                aliases=["선택", "목록", "pick"]
            ),
            ClarificationOption(
                key="scope:project_progress",
                label="프로젝트 전체 진행률로 보기",
                value={"intent_override": "STATUS_METRIC", "metric": "completion_rate"},
                aliases=["전체", "프로젝트"]
            ),
        ],
        allow_free_text=False
    )


def q_backlog_empty(intent: str = "BACKLOG_LIST") -> ClarificationQuestion:
    """
    Template: BACKLOG_LIST with empty backlog.

    Question ID: backlog.empty
    Trigger: Empty (query returned 0 rows)
    """
    return ClarificationQuestion(
        question_id="backlog.empty",
        intent=intent,
        message="현재 등록된 백로그가 없어요. 무엇을 도와드릴까요?",
        options=[
            ClarificationOption(
                key="backlog:show_templates",
                label="샘플 유저스토리 템플릿 3개 보기 (추천)",
                value={"fallback_mode": "show_templates"},
                is_default=True,
                aliases=["템플릿", "샘플"]
            ),
            ClarificationOption(
                key="backlog:how_to_create",
                label="백로그 입력 위치/절차 안내",
                value={"fallback_mode": "how_to_create"},
                aliases=["입력", "생성", "절차"]
            ),
            ClarificationOption(
                key="backlog:draft_from_epic_feature",
                label="Epic/Feature 기반 초안 만들기",
                value={"fallback_mode": "draft_from_epic_feature"},
                aliases=["초안", "epic", "feature"]
            ),
        ]
    )


def q_risk_empty(intent: str = "RISK_ANALYSIS") -> ClarificationQuestion:
    """
    Template: RISK_ANALYSIS with no risks.

    Question ID: risk.empty
    Trigger: Empty (no risks found)
    """
    return ClarificationQuestion(
        question_id="risk.empty",
        intent=intent,
        message="현재 리스크로 분류된 항목이 없어요. 어떤 기준으로 리스크를 찾아볼까요?",
        options=[
            ClarificationOption(
                key="risk:derive_from_blockers",
                label="BLOCKED/HIGH 이슈를 리스크로 추정 (추천)",
                value={"fallback_mode": "derive_from_blockers"},
                is_default=True,
                aliases=["block", "blocked", "막힘", "장애"]
            ),
            ClarificationOption(
                key="risk:keyword_scan",
                label="키워드(지연/품질/예산) 기반으로 탐색",
                value={"fallback_mode": "keyword_scan"},
                aliases=["키워드", "지연", "품질", "예산"]
            ),
            ClarificationOption(
                key="risk:label_policy",
                label="리스크 라벨링 정책 안내",
                value={"fallback_mode": "label_policy"},
                aliases=["정책", "라벨", "label"]
            ),
        ]
    )


def q_taskdue_empty(intent: str = "TASK_DUE_THIS_WEEK") -> ClarificationQuestion:
    """
    Template: TASK_DUE_THIS_WEEK with no tasks.

    Question ID: taskdue.empty
    Trigger: Empty (no tasks due this week)
    """
    return ClarificationQuestion(
        question_id="taskdue.empty",
        intent=intent,
        message="이번 주 마감 작업이 없어요. 범위를 바꿔볼까요?",
        options=[
            ClarificationOption(
                key="taskdue:next_2_weeks",
                label="다음 2주 마감으로 확장 (추천)",
                value={"time_window_days": 14},
                is_default=True,
                aliases=["2주", "14"]
            ),
            ClarificationOption(
                key="taskdue:all_open",
                label="미완료 전체 작업 보기",
                value={"fallback_mode": "all_open"},
                aliases=["미완료", "전체"]
            ),
            ClarificationOption(
                key="taskdue:by_assignee",
                label="담당자별로 보기",
                value={"fallback_mode": "by_assignee"},
                aliases=["담당", "사람", "assignee"]
            ),
        ]
    )


def q_my_tasks_empty(intent: str = "MY_TASKS") -> ClarificationQuestion:
    """
    Template: MY_TASKS with no assigned tasks.

    Question ID: mytasks.empty
    Trigger: Empty (user has no tasks)
    """
    return ClarificationQuestion(
        question_id="mytasks.empty",
        intent=intent,
        message="현재 배정된 작업이 없어요. 다음 중 어떤 것을 볼까요?",
        options=[
            ClarificationOption(
                key="mytasks:team_tasks",
                label="팀 전체 작업 현황 (추천)",
                value={"scope": "team", "intent_override": "TASK_LIST"},
                is_default=True,
                aliases=["팀", "전체"]
            ),
            ClarificationOption(
                key="mytasks:available_tasks",
                label="배정 가능한 작업 목록",
                value={"filter": "unassigned"},
                aliases=["배정", "unassigned"]
            ),
            ClarificationOption(
                key="mytasks:completed_tasks",
                label="내가 완료한 작업",
                value={"filter": "completed"},
                aliases=["완료", "completed"]
            ),
        ]
    )


def q_userstory_no_sprint(intent: str = "USER_STORY_LIST") -> ClarificationQuestion:
    """
    Template: USER_STORY_LIST with no active sprint.

    Question ID: userstory.no_sprint
    Trigger: Missing Scope (no active sprint for user stories)
    """
    return ClarificationQuestion(
        question_id="userstory.no_sprint",
        intent=intent,
        message="활성 스프린트가 없어서 유저스토리를 조회할 수 없어요. 어떻게 볼까요?",
        options=[
            ClarificationOption(
                key="userstory:all_backlog",
                label="전체 백로그에서 보기 (추천)",
                value={"scope": "backlog", "intent_override": "BACKLOG_LIST"},
                is_default=True,
                aliases=["백로그", "전체"]
            ),
            ClarificationOption(
                key="userstory:last_sprint",
                label="최근 완료 스프린트 기준",
                value={"fallback_mode": "last_completed_sprint"},
                aliases=["최근", "완료"]
            ),
            ClarificationOption(
                key="userstory:create_sprint",
                label="스프린트 생성 가이드",
                value={"action": "show_sprint_creation_guide"},
                aliases=["생성", "가이드"]
            ),
        ]
    )


def q_status_metric_ambiguous(intent: str = "STATUS_METRIC", query: str = "진행률") -> ClarificationQuestion:
    """
    Template: STATUS_METRIC with ambiguous metric request.

    Question ID: status.ambiguous_metric
    Trigger: Ambiguous (user asked for "진행률" without specifying)
    """
    return ClarificationQuestion(
        question_id="status.ambiguous_metric",
        intent=intent,
        message=f"'{query}'에 대해 어떤 기준으로 볼까요?",
        options=[
            ClarificationOption(
                key="metric:sprint_progress",
                label="현재 스프린트 진행률 (추천)",
                value={"metric": "sprint_progress", "intent_override": "SPRINT_PROGRESS"},
                is_default=True,
                aliases=["스프린트"]
            ),
            ClarificationOption(
                key="metric:project_completion",
                label="프로젝트 전체 완료율",
                value={"metric": "project_completion"},
                aliases=["프로젝트", "전체"]
            ),
            ClarificationOption(
                key="metric:task_completion",
                label="태스크 완료율",
                value={"metric": "task_completion"},
                aliases=["태스크", "작업"]
            ),
        ]
    )


# =============================================================================
# Factory Registry
# =============================================================================

CLARIFICATION_FACTORY = {
    "sprint.no_active_sprint": q_sprint_no_active,
    "backlog.empty": q_backlog_empty,
    "risk.empty": q_risk_empty,
    "taskdue.empty": q_taskdue_empty,
    "mytasks.empty": q_my_tasks_empty,
    "userstory.no_sprint": q_userstory_no_sprint,
    "status.ambiguous_metric": q_status_metric_ambiguous,
}


def get_factory(question_id: str):
    """Get a clarification factory by ID."""
    return CLARIFICATION_FACTORY.get(question_id)


def list_factories() -> list:
    """List all available factory IDs."""
    return list(CLARIFICATION_FACTORY.keys())
```

---

### 8.5 intent_handlers.py (MODIFY) - 4개 Intent 동시 적용

**Location**: `/llm-service/intent_handlers.py`

**B-Plan 핵심**: "데이터 없음"을 감지하면 `contract.has_data=False`로 하고, clarification을 붙여서 반환.

#### 8.5.1 SPRINT_PROGRESS Handler

```python
from clarification_factory import q_sprint_no_active
from response_contract import ResponseContract

def handle_sprint_progress(ctx):
    """
    Handle SPRINT_PROGRESS intent with P3.5 B-PLAN clarification.

    B-PLAN FLOW:
    1. Check for clarification context patch (fallback_mode, sprint_id)
    2. If no active sprint and not clarification-resolved -> return clarification
    3. Normal processing
    """
    executor = get_status_query_executor()
    session_ctx = ctx.session_state.get("context", {})

    # Check clarification patch
    fallback_mode = session_ctx.get("fallback_mode")
    explicit_sprint_id = session_ctx.get("sprint_id")

    sprint = None

    # Case 1: Explicit sprint_id from clarification
    if explicit_sprint_id:
        sprint = _get_sprint_by_id(executor, ctx.project_id, explicit_sprint_id)

    # Case 2: Fallback mode from clarification
    elif fallback_mode == "last_completed_sprint":
        sprint = _get_last_completed_sprint(executor, ctx.project_id)

    # Case 3: Default - try active sprint
    else:
        sprint = _get_active_sprint(executor, ctx.project_id)

    # NO SPRINT -> Clarification
    if sprint is None:
        return ResponseContract(
            intent="SPRINT_PROGRESS",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
            data={"sprint": None, "stories": [], "metrics": {}, "burndown": [], "velocity_history": [], "health": {}},
            success=True,
            has_data=False,  # B-PLAN: distinguish from failure
            clarification=q_sprint_no_active(),
            tips=["1~3 중 번호로 답하면 바로 진행상황을 계산해 드려요."],
            provenance="PostgreSQL",
        )

    # ... normal processing (metrics, burndown, health)
    # Return ResponseContract with success=True, has_data=True
```

#### 8.5.2 BACKLOG_LIST Handler

```python
from clarification_factory import q_backlog_empty

def handle_backlog_list(ctx):
    """
    Handle BACKLOG_LIST intent with P3.5 B-PLAN clarification.
    """
    executor = get_status_query_executor()
    session_ctx = ctx.session_state.get("context", {})
    mode = session_ctx.get("fallback_mode")

    result = executor.execute_raw_query(BACKLOG_ITEMS_QUERY, {"project_id": ctx.project_id})
    if not result.success:
        return ResponseContract(
            intent="BACKLOG_LIST",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
            data={},
            success=False,  # Query failure
            has_data=False,
            provenance="Unavailable",
        )

    items = result.data or []

    # EMPTY BACKLOG -> Check fallback_mode first
    if len(items) == 0:
        data = {"items": [], "count": 0}

        # User chose show_templates
        if mode == "show_templates":
            data["templates"] = [
                "As a PM, I want to register backlog items so that sprint planning is possible.",
                "As a developer, I want to see my assigned tasks so that I can prioritize work.",
                "As a team, we want sprint health indicators so that we can detect risks early."
            ]
            return ResponseContract(
                intent="BACKLOG_LIST",
                reference_time=_get_kst_reference_time(),
                scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
                data=data,
                success=True,
                has_data=True,  # Templates are meaningful data
                provenance="PostgreSQL",
                tips=["원하면 위 템플릿을 그대로 백로그로 변환하는 규칙 기반 생성도 붙일 수 있어요."]
            )

        if mode == "how_to_create":
            data["guide"] = {
                "steps": [
                    "1. 프로젝트 > 백로그 메뉴로 이동",
                    "2. '+ 유저스토리 추가' 버튼 클릭",
                    "3. 제목과 설명 입력 후 저장",
                ],
            }
            return ResponseContract(
                intent="BACKLOG_LIST",
                reference_time=_get_kst_reference_time(),
                scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
                data=data,
                success=True,
                has_data=True,
                provenance="Guide",
            )

        # No fallback_mode -> Ask clarification
        return ResponseContract(
            intent="BACKLOG_LIST",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
            data=data,
            success=True,
            has_data=False,
            clarification=q_backlog_empty(),
            tips=["번호로 선택하면, 다음 응답부터 바로 도움이 되는 형태로 보여드려요."],
            provenance="PostgreSQL",
        )

    # Non-empty: normal response
    return ResponseContract(
        intent="BACKLOG_LIST",
        reference_time=_get_kst_reference_time(),
        scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
        data={"items": [item_to_dict(i) for i in items], "count": len(items)},
        success=True,
        has_data=True,
        provenance="PostgreSQL",
    )
```

#### 8.5.3 RISK_ANALYSIS Handler

```python
from clarification_factory import q_risk_empty
from risk_model import is_risk_by_keyword, derive_risks_from_blockers, map_issue_to_risk, generate_risk_summary, RiskSource

def handle_risk_analysis(ctx):
    """
    Handle RISK_ANALYSIS intent with P3.5 B-PLAN clarification.
    """
    executor = get_status_query_executor()
    session_ctx = ctx.session_state.get("context", {})
    mode = session_ctx.get("fallback_mode")

    # Default: risk-labeled issues
    issues = _load_issues(executor, ctx.project_id)
    risks = [map_issue_to_risk(i, RiskSource.ISSUE_RISK_TYPE)
             for i in issues if i.get("type") == "RISK"]

    # Fallback: derive from blockers
    if len(risks) == 0 and mode == "derive_from_blockers":
        blockers = _load_blockers(executor, ctx.project_id)
        risks = derive_risks_from_blockers(blockers)

    # Fallback: keyword scan
    if len(risks) == 0 and mode == "keyword_scan":
        risks = [map_issue_to_risk(i, RiskSource.ISSUE_RISK_KEYWORD)
                for i in issues if is_risk_by_keyword(i.get("title",""), i.get("description",""))]

    # Fallback: show policy guide
    if len(risks) == 0 and mode == "label_policy":
        return ResponseContract(
            intent="RISK_ANALYSIS",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
            data={
                "risks": [],
                "summary": {"total": 0},
                "policy": {
                    "recommended_labels": ["RISK", "BLOCKER", "DELAY_RISK", "QUALITY_RISK"],
                    "rule": "이슈 생성 시 type=RISK 또는 label=RISK 부여, severity HIGH 이상 필수",
                }
            },
            success=True,
            has_data=True,  # Policy is meaningful data
            provenance="PostgreSQL",
            tips=["원하면 이 정책을 UI 폼/DB 제약으로 강제할 수 있어요."]
        )

    # Still no risks -> Ask clarification
    if len(risks) == 0:
        return ResponseContract(
            intent="RISK_ANALYSIS",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
            data={"risks": [], "summary": {"total": 0}},
            success=True,
            has_data=False,
            clarification=q_risk_empty(),
            tips=["1~3 중 선택하면, 바로 다른 기준으로 리스크를 탐색해 드려요."],
            provenance="PostgreSQL",
        )

    # Non-empty: normal response
    summary = generate_risk_summary(risks)
    return ResponseContract(
        intent="RISK_ANALYSIS",
        reference_time=_get_kst_reference_time(),
        scope={"project_id": ctx.project_id, "project_name": ctx.project_name},
        data={"risks": [r.__dict__ for r in risks], "summary": summary},
        success=True,
        has_data=True,
        provenance="PostgreSQL",
    )
```

#### 8.5.4 TASK_DUE_THIS_WEEK Handler

```python
from clarification_factory import q_taskdue_empty

def handle_tasks_due_this_week(ctx):
    """
    Handle TASK_DUE_THIS_WEEK intent with P3.5 B-PLAN clarification.

    B-PLAN: time_window_days patch is the cleanest pattern here.
    """
    executor = get_status_query_executor()
    session_ctx = ctx.session_state.get("context", {})

    window_days = session_ctx.get("time_window_days", 7)
    mode = session_ctx.get("fallback_mode")

    tasks = _load_tasks_due(executor, ctx.project_id, days=window_days)

    if len(tasks) == 0:
        # If user selected all_open, fetch open tasks
        if mode == "all_open":
            tasks = _load_open_tasks(executor, ctx.project_id)
            return ResponseContract(
                intent="TASK_DUE_THIS_WEEK",
                reference_time=_get_kst_reference_time(),
                scope={"project_id": ctx.project_id, "project_name": ctx.project_name, "window_days": "all"},
                data={"tasks": [task_to_dict(t) for t in tasks], "count": len(tasks)},
                success=True,
                has_data=len(tasks) > 0,
                provenance="PostgreSQL",
            )

        if mode == "by_assignee":
            tasks = _load_tasks_grouped_by_assignee(executor, ctx.project_id)
            return ResponseContract(
                intent="TASK_DUE_THIS_WEEK",
                reference_time=_get_kst_reference_time(),
                scope={"project_id": ctx.project_id, "project_name": ctx.project_name, "group_by": "assignee"},
                data={"tasks_by_assignee": tasks},
                success=True,
                has_data=len(tasks) > 0,
                provenance="PostgreSQL",
            )

        # No fallback_mode -> Ask clarification
        return ResponseContract(
            intent="TASK_DUE_THIS_WEEK",
            reference_time=_get_kst_reference_time(),
            scope={"project_id": ctx.project_id, "project_name": ctx.project_name, "window_days": window_days},
            data={"tasks": [], "count": 0},
            success=True,
            has_data=False,
            clarification=q_taskdue_empty(),
            tips=["번호로 선택하면, 범위를 바꿔 다시 조회해 드려요."],
            provenance="PostgreSQL",
        )

    # Non-empty: normal response
    return ResponseContract(
        intent="TASK_DUE_THIS_WEEK",
        reference_time=_get_kst_reference_time(),
        scope={"project_id": ctx.project_id, "project_name": ctx.project_name, "window_days": window_days},
        data={"tasks": [task_to_dict(t) for t in tasks], "count": len(tasks)},
        success=True,
        has_data=True,
        provenance="PostgreSQL",
    )
```

---

### 8.6 response_renderer.py (MODIFY) - B-Plan 출력 포맷

**Location**: `/llm-service/response_renderer.py`

**B-Plan 핵심**: clarification이 있으면 질문 + 선택지 + "번호로 답해달라" 안내를 **항상 고정 포맷**으로 출력

```python
"""
response_renderer.py - B-PLAN CLARIFICATION RENDERING

B-PLAN FORMAT:
❓ [Question]

1) Option 1 (추천)
2) Option 2
3) Option 3

👉 1~3 중 **번호**로 답해 주세요.

This format is CRITICAL because:
- Users immediately recognize "1 입력하면 되는구나"
- Repeatable UI pattern > LLM's pretty sentences
- Higher recovery rate
"""

from typing import List
from response_contract import ResponseContract

def render(contract: ResponseContract) -> str:
    """
    Render ResponseContract to user-facing text.

    B-PLAN RENDERING ORDER:
    1. If clarification exists -> render clarification ONLY
    2. If failure (success=False) -> render failure message
    3. Normal response rendering
    """

    # =========================================================================
    # B-PLAN: CLARIFICATION TAKES PRECEDENCE
    # =========================================================================
    if contract.has_clarification():
        return _render_clarification_block(contract)

    # =========================================================================
    # FAILURE VS EMPTY DISTINCTION
    # =========================================================================
    if not contract.success:
        body = "⚠️ 데이터를 불러올 수 없어요(조회 실패)."
        return _compose(contract, body)

    # =========================================================================
    # NORMAL RESPONSE
    # =========================================================================
    body = _render_main_body(contract)

    # Provenance footer
    if contract.provenance:
        body += f"\n\n— 데이터 출처: {contract.provenance}"

    return _compose(contract, body)


def _render_clarification_block(contract: ResponseContract) -> str:
    """
    Render clarification question with numbered options.

    B-PLAN FORMAT (FIXED):
    ❓ [Question]

    1) Option 1 (추천)
    2) Option 2
    3) Option 3

    👉 1~N 중 **번호**로 답해 주세요.
    """
    cq = contract.clarification
    lines = []

    # Empty line for visual separation
    lines.append("")

    # Question
    lines.append("❓ " + cq.message)
    lines.append("")

    # Numbered options (1-based)
    for i, opt in enumerate(cq.options, start=1):
        suffix = ""
        if opt.is_default and "(추천)" not in opt.label:
            suffix = " (추천)"
        lines.append(f"{i}) {opt.label}{suffix}")

    lines.append("")
    lines.append(f"👉 1~{len(cq.options)} 중 **번호**로 답해 주세요.")

    # Tips if present
    if contract.tips:
        lines.append("")
        for tip in contract.tips[:2]:
            lines.append(f"💡 {tip}")

    return "\n".join(lines)


def _render_main_body(contract: ResponseContract) -> str:
    """Render main data body based on intent."""
    intent = contract.intent.lower()

    if intent == "sprint_progress":
        return _render_sprint_progress(contract.data)
    elif intent == "backlog_list":
        return _render_backlog_list(contract.data)
    elif intent == "risk_analysis":
        return _render_risk_analysis(contract.data)
    elif intent == "task_due_this_week":
        return _render_task_due(contract.data)
    else:
        return _render_generic(contract.data)


def _compose(contract: ResponseContract, body: str) -> str:
    """Compose final response with tips."""
    result = body

    if contract.tips:
        result += "\n\n"
        for tip in contract.tips[:2]:
            result += f"💡 {tip}\n"

    return result.strip()


# Intent-specific renderers (implementations omitted for brevity)
def _render_sprint_progress(data: dict) -> str:
    # ... implementation
    pass

def _render_backlog_list(data: dict) -> str:
    # ... implementation
    pass

def _render_risk_analysis(data: dict) -> str:
    # ... implementation
    pass

def _render_task_due(data: dict) -> str:
    # ... implementation
    pass

def _render_generic(data: dict) -> str:
    # ... implementation
    pass
```

---

### 8.7 tests/test_clarification_flow_regression.py (NEW FILE)

**Location**: `/llm-service/tests/test_clarification_flow_regression.py`

**B-Plan 핵심**: "되묻기 생성"이 아니라 **"복구 성공"**을 테스트

```python
"""
P3.5 B-PLAN Regression Tests: Clarification Flow

B-PLAN TEST PHILOSOPHY:
- Test "RECOVERY SUCCESS", not just "clarification generation"
- Two-step tests:
  1. Empty/missing -> clarification MUST appear
  2. User "1" -> recovery MUST succeed with meaningful data

PURPOSE:
- Prevent STATUS_METRIC blackhole regression
- Ensure 1-turn recovery works end-to-end
"""

import pytest
from unittest.mock import Mock, patch

from clarification import ClarificationQuestion, ClarificationOption, PendingClarification
from clarification_factory import (
    q_sprint_no_active,
    q_backlog_empty,
    q_risk_empty,
    q_taskdue_empty,
)


class TestClarificationTriggered:
    """Test that clarification is triggered correctly for each intent."""

    def test_sprint_no_active_triggers_clarification(self, client):
        """Given: active sprint query returns none
        When: user asks for sprint progress
        Then: clarification should be triggered
        """
        client.seed_active_sprint(None)

        r1 = client.ask("현재 스프린트 진행 상황")

        assert r1.contract.clarification is not None
        assert r1.contract.clarification.question_id == "sprint.no_active_sprint"
        assert r1.contract.success is True
        assert r1.contract.has_data is False

    def test_backlog_empty_triggers_clarification(self, client):
        """Given: backlog is empty
        When: user asks for backlog list
        Then: clarification should be triggered
        """
        client.seed_empty_backlog()

        r1 = client.ask("백로그 목록 보여줘")

        assert r1.contract.clarification is not None
        assert r1.contract.clarification.question_id == "backlog.empty"

    def test_risk_empty_triggers_clarification(self, client):
        """Given: no risks in project
        When: user asks for risk analysis
        Then: clarification should be triggered
        """
        client.seed_empty_risks()

        r1 = client.ask("리스크 분석해줘")

        assert r1.contract.clarification is not None
        assert r1.contract.clarification.question_id == "risk.empty"

    def test_taskdue_empty_triggers_clarification(self, client):
        """Given: no tasks due this week
        When: user asks for due tasks
        Then: clarification should be triggered
        """
        client.seed_empty_taskdue()

        r1 = client.ask("이번 주 마감 작업")

        assert r1.contract.clarification is not None
        assert r1.contract.clarification.question_id == "taskdue.empty"


class TestClarificationRecovery:
    """Test that clarification selection recovers correctly."""

    def test_sprint_clarification_choice_1_recovers(self, client):
        """Given: clarification triggered for sprint
        When: user answers "1" (last completed sprint)
        Then: recovery should succeed with actual data
        """
        # Setup: no active sprint, but has completed sprint with data
        client.seed_active_sprint(None)
        client.seed_last_completed_sprint({"id": "s2", "name": "Sprint 2"})
        client.seed_sprint_stories("s2", [{"id": 1, "status": "DONE"}])

        # Step 1: Initial request -> clarification
        r1 = client.ask("현재 스프린트 진행 상황")
        assert r1.contract.clarification is not None

        # Step 2: User answers "1" -> recovery
        r2 = client.ask("1")

        # CRITICAL ASSERTIONS
        assert r2.contract.clarification is None  # No more clarification
        assert r2.contract.success is True
        assert r2.contract.has_data is True
        assert r2.contract.data.get("sprint") is not None

    def test_backlog_clarification_choice_1_shows_templates(self, client):
        """Given: clarification triggered for empty backlog
        When: user answers "1" (show templates)
        Then: recovery should show template data
        """
        client.seed_empty_backlog()

        r1 = client.ask("백로그 보여줘")
        assert r1.contract.clarification is not None

        r2 = client.ask("1")

        assert r2.contract.clarification is None
        assert r2.contract.success is True
        assert r2.contract.has_data is True
        assert "templates" in r2.contract.data

    def test_taskdue_clarification_choice_1_extends_window(self, client):
        """Given: clarification triggered for empty taskdue
        When: user answers "1" (extend to 2 weeks)
        Then: recovery should query with 14-day window
        """
        client.seed_empty_taskdue(days=7)
        client.seed_tasks_due(days=14, tasks=[{"id": 1, "title": "Task 1"}])

        r1 = client.ask("이번 주 마감 작업")
        assert r1.contract.clarification is not None

        r2 = client.ask("1")

        assert r2.contract.clarification is None
        assert r2.contract.success is True
        assert r2.contract.has_data is True
        assert len(r2.contract.data.get("tasks", [])) > 0


class TestClassifierBypass:
    """Test that classifier is bypassed when pending clarification exists."""

    def test_pending_clarification_bypasses_classifier(self, client):
        """Given: pending clarification exists
        When: user responds with "1"
        Then: classifier should NOT be called
        """
        client.seed_active_sprint(None)

        # Step 1: Trigger clarification
        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        # Step 2: Answer with "1"
        with patch.object(client, '_classify_intent') as mock_classify:
            r2 = client.ask("1")

            # CRITICAL: Classifier should not be called
            mock_classify.assert_not_called()

    def test_unmatched_input_reasks_without_classifier(self, client):
        """Given: pending clarification exists
        When: user responds with invalid input
        Then: same clarification should be shown again, classifier NOT called
        """
        client.seed_active_sprint(None)

        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None
        original_question_id = r1.contract.clarification.question_id

        with patch.object(client, '_classify_intent') as mock_classify:
            r2 = client.ask("무슨 말이야")  # Invalid input

            mock_classify.assert_not_called()
            assert r2.contract.clarification is not None
            assert r2.contract.clarification.question_id == original_question_id


class TestClarificationExpiration:
    """Test TTL expiration behavior."""

    def test_expired_clarification_is_cleared(self, client):
        """Given: pending clarification is expired (TTL passed)
        When: user sends any message
        Then: pending should be cleared, normal flow resumes
        """
        client.seed_active_sprint(None)

        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        # Simulate TTL expiration
        client.expire_pending_clarification()

        # Now any message should go through normal classification
        with patch.object(client, '_classify_intent', return_value="CASUAL") as mock_classify:
            r2 = client.ask("안녕")
            mock_classify.assert_called_once()


class TestAllIntentsParameterized:
    """Parameterized tests for all 4 intents."""

    @pytest.mark.parametrize("intent,seed_method,question_id", [
        ("SPRINT_PROGRESS", "seed_active_sprint_none", "sprint.no_active_sprint"),
        ("BACKLOG_LIST", "seed_empty_backlog", "backlog.empty"),
        ("RISK_ANALYSIS", "seed_empty_risks", "risk.empty"),
        ("TASK_DUE_THIS_WEEK", "seed_empty_taskdue", "taskdue.empty"),
    ])
    def test_all_intents_trigger_clarification(self, client, intent, seed_method, question_id):
        """All intents should trigger clarification when data is empty."""
        getattr(client, seed_method)()
        r = client.ask(f"trigger for {intent}")
        assert r.contract.clarification is not None
        assert r.contract.clarification.question_id == question_id

    @pytest.mark.parametrize("intent,seed_method,recovery_setup", [
        ("SPRINT_PROGRESS", "seed_active_sprint_none", "seed_last_completed_sprint_with_data"),
        ("BACKLOG_LIST", "seed_empty_backlog", None),  # Templates don't need extra setup
        ("RISK_ANALYSIS", "seed_empty_risks", "seed_blockers_for_risk_derivation"),
        ("TASK_DUE_THIS_WEEK", "seed_empty_taskdue", "seed_tasks_due_14_days"),
    ])
    def test_all_intents_recover_on_option_1(self, client, intent, seed_method, recovery_setup):
        """All intents should recover when user selects option 1."""
        getattr(client, seed_method)()
        if recovery_setup:
            getattr(client, recovery_setup)()

        r1 = client.ask(f"trigger for {intent}")
        assert r1.contract.clarification is not None

        r2 = client.ask("1")
        assert r2.contract.clarification is None
        assert r2.contract.success is True
        # has_data should be True (either real data or fallback data like templates/policy)
        assert r2.contract.has_data is True


# =============================================================================
# v2.1 Additional Tests (운영 안정성 강화)
# =============================================================================

class TestTopicChangeAbandon:
    """v2.1: Test abandon detection when user changes topic (R5)."""

    def test_explicit_cancel_abandons_pending(self, client):
        """Given: pending clarification exists
        When: user says "취소"
        Then: pending should be abandoned, normal flow resumes
        """
        client.seed_active_sprint(None)
        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        r2 = client.ask("취소")
        # Should go through normal classification
        assert "pending_clarification" not in client.session.state

    def test_different_topic_abandons_pending(self, client):
        """Given: pending clarification exists for sprint
        When: user asks about backlog (long sentence, no option match)
        Then: pending should be abandoned, backlog query should run
        """
        client.seed_active_sprint(None)
        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        with patch.object(client, '_classify_intent', return_value="BACKLOG_LIST") as mock:
            r2 = client.ask("백로그 목록을 보여주세요")
            mock.assert_called_once()  # Classifier was called (abandon happened)


class TestAliasMatchingProtection:
    """v2.1: Test alias matching doesn't over-match (R2)."""

    def test_short_alias_exact_match_only(self, client):
        """Given: alias is "전체"
        When: user says "전체적으로 좋아요"
        Then: should NOT match (partial word)
        """
        question = q_sprint_no_active()
        # "전체" is an alias for option 3

        # Should NOT match "전체적으로"
        opt = question.match_user_input("전체적으로 좋아요")
        assert opt is None or opt.key != "scope:project_progress"

    def test_alias_exact_match_works(self, client):
        """Given: alias is "전체"
        When: user says "전체"
        Then: should match
        """
        question = q_sprint_no_active()
        opt = question.match_user_input("전체")
        assert opt is not None
        assert opt.key == "scope:project_progress"


class TestEphemeralContextCleanup:
    """v2.1: Test ephemeral context doesn't leak to next query (R4)."""

    def test_ephemeral_cleared_after_recovery(self, client):
        """Given: user selected option and recovery succeeded
        When: next unrelated query
        Then: ephemeral context should be empty
        """
        client.seed_active_sprint(None)
        client.seed_last_completed_sprint({"id": "s2"})

        r1 = client.ask("스프린트 진행률")
        r2 = client.ask("1")  # Select last completed

        # Ephemeral should be cleared
        assert "ephemeral_context" not in client.session.state or \
               client.session.state.get("ephemeral_context") == {}


class TestSecondaryFallback:
    """v2.1: Test double clarification is prevented (R3)."""

    def test_secondary_fallback_when_first_fails(self, client):
        """Given: user selected option 1 but still no data
        When: handler returns another clarification
        Then: secondary fallback should kick in, no clarification
        """
        # Setup: no active, no completed sprint
        client.seed_active_sprint(None)
        client.seed_last_completed_sprint(None)

        r1 = client.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        r2 = client.ask("1")  # Select "last completed" but it doesn't exist

        # Should NOT get another clarification (secondary fallback)
        assert r2.contract.clarification is None
        # Should have tip explaining the auto-switch
        assert any("전체" in tip or "프로젝트" in tip for tip in (r2.contract.tips or []))


class TestConcurrencyIsolation:
    """v2.1: Test session isolation for concurrent users."""

    def test_pending_isolated_between_sessions(self, client_factory):
        """Given: two different users
        When: user A has pending, user B queries
        Then: user B should not see user A's pending
        """
        client_a = client_factory(user_id="user_a")
        client_b = client_factory(user_id="user_b")

        client_a.seed_active_sprint(None)
        r1 = client_a.ask("스프린트 진행률")
        assert r1.contract.clarification is not None

        # User B should have normal flow
        client_b.seed_active_sprint({"id": "s1"})
        r2 = client_b.ask("스프린트 진행률")
        assert r2.contract.clarification is None  # B has data


class TestAxisValidation:
    """v2.1: Test single-axis enforcement (R1)."""

    def test_invalid_axis_rejected(self):
        """ClarificationQuestion with invalid axis should raise."""
        with pytest.raises(ValueError, match="axis must be one of"):
            ClarificationQuestion(
                question_id="test",
                intent="TEST",
                message="This is a test question with reason",
                axis="invalid_axis",  # Invalid!
                options=[
                    ClarificationOption(key="a", label="A", value={}, is_default=True),
                    ClarificationOption(key="b", label="B", value={}),
                ]
            )


# =============================================================================
# Test Fixtures
# =============================================================================

@pytest.fixture
def client():
    """Create test client with seeding capabilities."""
    from test_utils import MockChatClient
    return MockChatClient()


@pytest.fixture
def client_factory():
    """Factory for creating multiple test clients (for concurrency tests)."""
    from test_utils import MockChatClient
    def _factory(user_id: str):
        return MockChatClient(user_id=user_id)
    return _factory
```

---

### 8.8 metrics.py (MODIFY)

**Location**: `/llm-service/metrics.py`

```python
"""
P3.5 B-PLAN Metrics

METRICS TO TRACK:
1. clarification_triggered_total{intent, question_id}
2. clarification_resolved_total{question_id}
3. clarification_abandoned_total{question_id} (TTL expire or different topic)
4. clarification_turns_to_resolution (histogram)

OPERATIONAL TARGETS:
- resolution_rate (1-turn) >= 70%
- abandoned_rate <= 15%
- trigger_rate: "too high = data model/process issue"
"""

from prometheus_client import Counter, Histogram

# =============================================================================
# P3.5 B-PLAN Metrics
# =============================================================================

CLARIFICATION_TRIGGERED_TOTAL = Counter(
    "llm_clarification_triggered_total",
    "Total clarification questions triggered",
    ["intent", "question_id"],
)

CLARIFICATION_RESOLVED_TOTAL = Counter(
    "llm_clarification_resolved_total",
    "Total clarification resolutions",
    ["question_id", "resolution_type"],  # "matched", "timeout", "abandoned"
)

CLARIFICATION_ABANDONED_TOTAL = Counter(
    "llm_clarification_abandoned_total",
    "Total clarification abandonments (TTL or topic change)",
    ["question_id"],
)

CLARIFICATION_TURNS_TO_RESOLUTION = Histogram(
    "llm_clarification_turns_to_resolution",
    "Number of turns to resolve clarification (target: 1)",
    ["intent"],
    buckets=[1, 2, 3, 5, 10],
)


def record_clarification_triggered(intent: str, question_id: str) -> None:
    """Record that a clarification was triggered."""
    CLARIFICATION_TRIGGERED_TOTAL.labels(
        intent=intent,
        question_id=question_id,
    ).inc()


def record_clarification_resolved(question_id: str, resolution_type: str, turns: int = 1) -> None:
    """Record that a clarification was resolved."""
    CLARIFICATION_RESOLVED_TOTAL.labels(
        question_id=question_id,
        resolution_type=resolution_type,
    ).inc()

    if resolution_type == "matched":
        # Extract intent from question_id (e.g., "sprint.no_active_sprint" -> "sprint")
        intent = question_id.split(".")[0] if "." in question_id else "unknown"
        CLARIFICATION_TURNS_TO_RESOLUTION.labels(intent=intent).observe(turns)


def record_clarification_abandoned(question_id: str) -> None:
    """Record that a clarification was abandoned."""
    CLARIFICATION_ABANDONED_TOTAL.labels(question_id=question_id).inc()


# =============================================================================
# P3.5 Alert Thresholds
# =============================================================================

P35_ALERT_THRESHOLDS = {
    # If resolution rate drops below 70%, UX is degrading
    "clarification_resolution_rate_low": {
        "metric": "clarification_resolution_rate",
        "threshold": 0.7,
        "operator": "<",
        "severity": "warning",
        "message": "Clarification resolution rate below 70% - check question quality",
    },
    # If trigger rate exceeds 30%, data model or process issues
    "clarification_trigger_rate_high": {
        "metric": "clarification_trigger_rate",
        "threshold": 0.3,
        "operator": ">",
        "severity": "warning",
        "message": "Clarification trigger rate exceeds 30% - investigate data model",
    },
    # If abandoned rate exceeds 15%, questions are confusing
    "clarification_abandoned_rate_high": {
        "metric": "clarification_abandoned_rate",
        "threshold": 0.15,
        "operator": ">",
        "severity": "warning",
        "message": "Clarification abandoned rate exceeds 15% - review question clarity",
    },
}
```

---

## 9. B-Plan 실행 순서 (안전하게 일괄 적용)

**B-Plan은 변경 폭이 크므로, 순서가 안전성의 핵심입니다.**

| Step | Task | Verification | Rollback Point |
|------|------|--------------|----------------|
| 1 | **Contract + Renderer 먼저** | clarification 출력만 해도 실패하지 않음 | render() 조건부 skip |
| 2 | **chat_workflow_v2 상태 머신** | pending 저장/해소/TTL 처리 | feature flag off |
| 3 | **clarification_factory.py** | 4개 질문 템플릿 완성 | N/A (data only) |
| 4 | **SPRINT_PROGRESS 핸들러** | empty 감지 → clarification 반환 | 핸들러 조건부 skip |
| 5 | **BACKLOG_LIST 핸들러** | empty 감지 → clarification 반환 | 핸들러 조건부 skip |
| 6 | **RISK_ANALYSIS 핸들러** | empty 감지 → clarification 반환 | 핸들러 조건부 skip |
| 7 | **TASK_DUE_THIS_WEEK 핸들러** | empty 감지 → clarification 반환 | 핸들러 조건부 skip |
| 8 | **Regression Tests** | "1 입력 → 복구" 테스트 통과 | 실패 시 4-7 롤백 |
| 9 | **Metrics 연동** | Prometheus 엔드포인트 확인 | N/A |
| 10 | **Canary 5%** | resolution_rate >= 70% | 전체 롤백 |

---

## 10. Success Criteria

### Quantitative

| Metric | Target | Critical Threshold |
|--------|--------|-------------------|
| **clarification_resolution_rate** | ≥ 70% | < 50% = rollback |
| **clarification_trigger_rate** | ≤ 30% | > 50% = investigate |
| **avg_turns_to_resolution** | ≤ 1.2 | > 2.0 = UX review |
| **clarification_abandon_rate** | ≤ 15% | > 40% = question quality issue |

### Qualitative

- ✅ Empty 응답이 막다른 길이 아니다
- ✅ 되묻기는 항상 선택지가 있다 (2-4개)
- ✅ 사용자가 답하면 즉시 재실행된다
- ✅ 1턴 해결률이 높다 (70%+)
- ✅ STATUS_METRIC 블랙홀로 떨어지지 않는다 (classifier bypass)

---

## 11. Rollback Plan

### Trigger Conditions

1. `clarification_resolution_rate < 50%` for 1 hour
2. `clarification_trigger_rate > 50%` for 30 minutes
3. User complaints about "stuck in questions"
4. STATUS_METRIC blackhole regression detected

### Rollback Steps

```bash
# 1. Disable P3.5 via feature flag
export P35_CLARIFICATION_ENABLED=false

# 2. Restart services
docker-compose restart llm-service

# 3. Verify fallback to P3 behavior
# (empty data -> recovery_plan instead of clarification)
```

### Post-Rollback Analysis

1. Check which templates triggered most frequently
2. Review unmatched user inputs (improve aliases?)
3. Analyze abandon patterns (TTL too short? Questions confusing?)
4. Check if classifier bypass worked correctly

---

## 12. Dependencies

### P3.5 Depends On

| Dependency | Status | Required For |
|------------|--------|--------------|
| P3 `ResponseContract` | ✅ Implemented | Adding `clarification` + `has_data` fields |
| P3 `Explainability` | ✅ Implemented | Explaining why clarification triggered |
| P3 `RecoveryPlan` | ✅ Implemented | Fallback when clarification fails |
| Session state persistence | ⚠️ Verify | Storing `pending_clarification` |

### Session State Requirements

P3.5 requires conversation state to persist between turns. Verify:

```python
# Must work:
session.state["pending_clarification"] = {...}
session.save_state()

# Next turn:
assert session.state.get("pending_clarification") is not None
```

---

## 13. Appendix: Complete File Structure

```
llm-service/
├── clarification.py                         # NEW: P3.5 data models (with aliases)
├── clarification_factory.py                 # NEW: Standard question factories
├── response_contract.py                     # MODIFIED: +clarification, +has_data
├── chat_workflow_v2.py                      # MODIFIED: B-PLAN state machine
├── intent_handlers.py                       # MODIFIED: 4 intents with clarification
├── response_renderer.py                     # MODIFIED: B-PLAN output format
├── metrics.py                               # MODIFIED: P3.5 metrics
├── tests/
│   └── test_clarification_flow_regression.py  # NEW: Recovery success tests
└── docs/
    └── P3.5_SELF_HEALING_CLARIFICATION.md   # THIS FILE
```

---

## 14. 운영 가이드: "성공률 70%"를 진짜로 달성하는 법 (v2.1)

### 핵심 원칙: 옵션 구성이 성공률을 좌우한다

지표 목표(1턴 해결률 70%+)는 **질문 문구가 아니라 옵션 구성의 성공확률**이 좌우합니다.

| 옵션 순서 | 원칙 | 예시 |
|-----------|------|------|
| **1번** | 데이터가 있을 확률이 가장 높은 경로 | 최근 완료 스프린트 (실패 거의 없음) |
| **2번** | 사용자가 진짜 원할 때 선택 | 특정 스프린트 선택 |
| **3번** | 실패하지 않는 안전 옵션 | 가이드/정책/템플릿 |

### Intent별 현실적 튜닝 포인트

#### Sprint: "최근 완료 스프린트"가 없을 수 있다

팀 초기 프로젝트에서 흔합니다. **추천 옵션 재배열**:

```
1) 스프린트 생성 가이드 (팀 초기일 때 확실히 성공)
2) 프로젝트 전체 기준 (데이터가 있을 확률 높음)
3) 최근 완료 스프린트 (데이터가 있어야 의미 있음)
```

> 운영 데이터 축적 후, 완료 스프린트가 대부분 있으면 다시 순서 조정

#### Backlog empty: 템플릿 → '바로 생성' 연결 고려

P3.5 범위에서는 템플릿 표시로 충분하지만, 다음 단계(P4)에서:
- "원하시면 '생성:1' 입력으로 바로 등록" 같은 단축 명령 추가 가능

#### Risk empty: derive_from_blockers는 "근거 표시"가 중요

BLOCKED/HIGH에서 리스크 추정 시 사용자 신뢰 확보:

```python
risk_item = {
    "source": "blocked_issue",
    "rule": "status=BLOCKED OR severity=HIGH",
    "confidence": "medium"
}
```

#### TaskDue empty: window 확장은 "휴일/타임존 경계" 주의

- `window_days=7` 대신 **KST 주간 경계(start, end)**를 scope에 명시
- `next_2_weeks`도 "end + 14일"처럼 명확히

### 추천(default) 옵션 운영 정책

> **추천은 코드가 아니라 운영 데이터로 주기적으로 바뀌어야 하는 대상**

초기 도입기: "가이드/템플릿 경로"가 성공률이 높을 수 있음
→ factory의 `is_default` 순서를 데이터 기반으로 조정

### has_data 일관성 기준 (v2.1 - R7)

모든 핸들러에서 동일한 기준 적용:

| has_data | 정의 |
|----------|------|
| **True** | 사용자에게 "다음 액션"을 가능하게 하는 정보 존재 (실 데이터, 가이드, 템플릿) |
| **False** | 사용자가 다음 행동 불가 (0 rows + 대안도 없음) → clarification 필요 |

---

## 15. B-Plan의 본질 (v2.1 철학 정리)

> **B-Plan을 선택한 건 기능 욕심이 아니라 운영 철학입니다.**

1. 데이터가 없을 때 **"없다"로 끝내지 않고**
2. 사용자에게 **"어떤 방향으로 복구할지"를 묻고**
3. 그 답을 **시스템 상태로 고정해서**
4. 다음 응답부터 **정상 궤도로 복귀**시키는 것

이게 되면, AI PMS는 **"조회 도구"가 아니라 프로세스 촉진자**가 됩니다.

---

## 16. Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2026-02-03 | AI Assistant | Initial P3.5 specification |
| 2.0 | 2026-02-04 | AI Assistant | **B-PLAN overhaul**: classifier bypass, has_data field, aliases support, 4-intent simultaneous rollout, recovery success tests, execution order |
| 2.1 | 2026-02-04 | AI Assistant | **운영 안정성 강화**: (R1) axis 필드 single-axis 강제, (R2) alias 오매칭 방지 word boundary, (R3) 2차 fallback 자동 전환, (R4) ephemeral_context 분리, (R5) topic change abandon detection, (R6) metrics 레이블 세분화, (R7) has_data 일관성 기준, (R8) message 이유 강제, 6개 추가 테스트, 70% 성공률 운영 가이드 |
