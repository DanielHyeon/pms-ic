"""
Neo4j ê¸°ë°˜ GraphRAG ì„œë¹„ìŠ¤
ë²¡í„° ê²€ìƒ‰ + ê·¸ë˜í”„ ê´€ê³„ë¥¼ Neo4j ë‹¨ì¼ DBì—ì„œ ì²˜ë¦¬

ì°¸ì¡°: https://github.com/gongwon-nayeon/graphrag-tools-retriever
"""

import logging
import os
import uuid
from typing import Dict, List, Optional
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer

from document_parser import MinerUDocumentParser, LayoutAwareChunker

logger = logging.getLogger(__name__)

class ToolsRetriever:
    """ìƒí™©ì— ë”°ë¼ ê²€ìƒ‰ ì „ëµì„ ì„ íƒí•˜ëŠ” ê°„ë‹¨í•œ ToolsRetriever."""

    def __init__(self, search_fn):
        self.search_fn = search_fn

    def retrieve(
        self,
        query: str,
        top_k: int = 3,
        filter_metadata: Optional[Dict] = None,
    ) -> List[Dict]:
        strategy = self._select_strategy(query, filter_metadata)
        use_graph_expansion = strategy == "graph"
        logger.info("ToolsRetriever strategy=%s, use_graph_expansion=%s", strategy, use_graph_expansion)
        return self.search_fn(
            query=query,
            top_k=top_k,
            filter_metadata=filter_metadata,
            use_graph_expansion=use_graph_expansion,
        )

    def _select_strategy(self, query: str, filter_metadata: Optional[Dict]) -> str:
        query_lower = query.lower()

        graph_keywords = [
            "ê´€ê³„", "ì—°ê´€", "ì—°ê³„", "ì—°ê²°", "íë¦„", "ìˆœì„œ", "ë‹¨ê³„", "í”„ë¡œì„¸ìŠ¤",
            "ì´ì „", "ë‹¤ìŒ", "ì „í›„", "ë§¥ë½", "ì—°ì†", "ì˜ì¡´",
        ]
        vector_keywords = ["ì •ì˜", "ì˜ë¯¸", "ë­", "ë¬´ì—‡", "ì„¤ëª…", "ê°œìš”", "ì°¨ì´", "ë¹„êµ"]
        domain_vector_keywords = [
            "í”Œë˜ë‹ í¬ì»¤", "planning poker", "ìŠ¤í¬ëŸ¼", "scrum", "xp",
            "extreme programming", "ì¹¸ë°˜", "kanban",
        ]

        # ê° í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ ë° ë¡œê¹…
        matched_graph = [kw for kw in graph_keywords if kw in query_lower]
        matched_vector = [kw for kw in vector_keywords if kw in query_lower]
        matched_domain = [kw for kw in domain_vector_keywords if kw in query_lower]

        logger.info(f"ğŸ” ToolsRetriever strategy selection for query: '{query}'")
        logger.info(f"  - Graph keywords matched: {matched_graph}")
        logger.info(f"  - Vector keywords matched: {matched_vector}")
        logger.info(f"  - Domain keywords matched: {matched_domain}")
        logger.info(f"  - Filter metadata: {filter_metadata}")
        logger.info(f"  - Query length: {len(query_lower)}")

        if any(keyword in query_lower for keyword in graph_keywords):
            logger.info("  âœ… Selected strategy: GRAPH (graph keywords matched)")
            return "graph"
        if any(keyword in query_lower for keyword in vector_keywords):
            logger.info("  âœ… Selected strategy: VECTOR (vector keywords matched)")
            return "vector"
        if any(keyword in query_lower for keyword in domain_vector_keywords):
            logger.info("  âœ… Selected strategy: VECTOR (domain keywords matched)")
            return "vector"
        if filter_metadata:
            logger.info("  âœ… Selected strategy: VECTOR (filter metadata present)")
            return "vector"
        if len(query_lower) <= 12:
            logger.info("  âœ… Selected strategy: VECTOR (short query)")
            return "vector"
        logger.info("  âœ… Selected strategy: GRAPH (default)")
        return "graph"


class RAGServiceNeo4j:
    """Neo4j ê¸°ë°˜ GraphRAG ì„œë¹„ìŠ¤ - ë²¡í„° + ê·¸ë˜í”„ í†µí•©"""

    def __init__(self):
        # Neo4j ì—°ê²° ì„¤ì •
        neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        neo4j_password = os.getenv("NEO4J_PASSWORD", "pmspassword123")

        logger.info(f"Connecting to Neo4j at {neo4j_uri}")
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embedding_device = os.getenv("EMBEDDING_DEVICE", "cpu")
        logger.info(f"Loading embedding model on device: {embedding_device}...")

        try:
            self.embedding_model = SentenceTransformer(
                'intfloat/multilingual-e5-large',
                device=embedding_device
            )
        except Exception as e:
            logger.warning(f"Failed to load embedding model on {embedding_device}: {e}")
            embedding_device = "cpu"
            self.embedding_model = SentenceTransformer(
                'intfloat/multilingual-e5-large',
                device=embedding_device
            )

        self.embedding_dim = 1024
        logger.info("Embedding model loaded successfully")

        # MinerU íŒŒì„œ ë° ì²­ì»¤ ì´ˆê¸°í™”
        use_mineru_model = os.getenv("USE_MINERU_MODEL", "true").lower() == "true"
        mineru_device = os.getenv("MINERU_DEVICE", "cpu")

        if use_mineru_model:
            logger.info("Loading MinerU2.5 model for advanced document parsing...")
            self.parser = MinerUDocumentParser(use_mock=False, device=mineru_device)
        else:
            logger.info("Using heuristic-based document parsing (mock mode)...")
            self.parser = MinerUDocumentParser(use_mock=True)

        self.chunker = LayoutAwareChunker(max_chunk_size=800, overlap=100)
        self.tools_retriever = ToolsRetriever(self._search_impl)

        # ì´ˆê¸° ì„¤ì •
        self._initialize_database()

    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸° ì„¤ì • (ì œì•½ì¡°ê±´, ì¸ë±ìŠ¤ ìƒì„±)"""
        with self.driver.session() as session:
            try:
                # 1. ìœ ë‹ˆí¬ ì œì•½ì¡°ê±´ ìƒì„±
                constraints = [
                    "CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.doc_id IS UNIQUE",
                    "CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE",
                    "CREATE CONSTRAINT IF NOT EXISTS FOR (cat:Category) REQUIRE cat.name IS UNIQUE",
                ]

                for constraint in constraints:
                    try:
                        session.run(constraint)
                    except Exception as e:
                        logger.debug(f"Constraint already exists or error: {e}")

                # 2. ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
                try:
                    session.run("""
                        CREATE VECTOR INDEX chunk_embeddings IF NOT EXISTS
                        FOR (c:Chunk)
                        ON c.embedding
                        OPTIONS {
                            indexConfig: {
                                `vector.dimensions`: $dimensions,
                                `vector.similarity_function`: 'cosine'
                            }
                        }
                    """, dimensions=self.embedding_dim)
                    logger.info("âœ… Vector index created/verified successfully")
                except Exception as e:
                    logger.warning(f"Vector index creation: {e}")

                logger.info("âœ… Neo4j database initialized successfully")

            except Exception as e:
                logger.error(f"Failed to initialize database: {e}", exc_info=True)

    def add_documents(self, documents: List[Dict[str, str]]) -> int:
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ Neo4jì— ì¶”ê°€"""
        success_count = 0
        for document in documents:
            if self.add_document(document):
                success_count += 1
        return success_count

    def add_document(self, document: Dict[str, str]) -> bool:
        """ë‹¨ì¼ ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ + ë²¡í„°ë¡œ ì¶”ê°€"""
        try:
            doc_id = document.get("id")
            content = document.get("content", "")
            metadata = document.get("metadata", {}) or {}

            if not doc_id or not content:
                logger.error("Document must have 'id' and 'content'")
                return False

            title = metadata.get("title") or metadata.get("file_name") or doc_id
            category = metadata.get("category", "general")
            file_type = metadata.get("file_type", "unknown")

            # êµ¬ì¡° íŒŒì‹± ë° ì²­í‚¹
            logger.info(f"Parsing document {doc_id} with MinerU...")
            blocks = self.parser.parse_document(content, metadata)
            chunks = self.chunker.chunk_blocks(blocks)

            logger.info(
                "Document %s parsed into %d blocks and %d chunks",
                doc_id,
                len(blocks),
                len(chunks),
            )

            with self.driver.session() as session:
                # 1. Document ë…¸ë“œ ìƒì„±
                session.run("""
                    MERGE (d:Document {doc_id: $doc_id})
                    SET d.title = $title,
                        d.content = $content,
                        d.file_type = $file_type,
                        d.file_path = $file_path,
                        d.created_at = $created_at
                """, doc_id=doc_id, title=title, content=content[:1000],
                           file_type=file_type,
                           file_path=metadata.get("file_path", ""),
                           created_at=metadata.get("created_at", ""))

                # 2. Category ë…¸ë“œì™€ ê´€ê³„ ìƒì„±
                session.run("""
                    MERGE (cat:Category {name: $category})
                    WITH cat
                    MATCH (d:Document {doc_id: $doc_id})
                    MERGE (d)-[:BELONGS_TO]->(cat)
                """, category=category, doc_id=doc_id)

                # 3. Chunk ë…¸ë“œë“¤ ìƒì„± ë° ê´€ê³„ ì„¤ì •
                chunk_ids = []
                for i, chunk_data in enumerate(chunks):
                    chunk_id = str(uuid.uuid4())
                    chunk_content = chunk_data["content"]
                    chunk_metadata = chunk_data["metadata"]

                    # ì„ë² ë”© ìƒì„±
                    embedding_text = f"passage: {chunk_content}"
                    embedding = self.embedding_model.encode(embedding_text).tolist()

                    # Chunk ë…¸ë“œ ìƒì„±
                    session.run("""
                        MERGE (c:Chunk {chunk_id: $chunk_id})
                        SET c.content = $content,
                            c.chunk_index = $chunk_index,
                            c.title = $title,
                            c.doc_id = $doc_id,
                            c.structure_type = $structure_type,
                            c.has_table = $has_table,
                            c.has_list = $has_list,
                            c.section_title = $section_title,
                            c.page_number = $page_number,
                            c.embedding = $embedding
                    """, chunk_id=chunk_id, content=chunk_content,
                               chunk_index=i, title=title, doc_id=doc_id,
                               structure_type=chunk_metadata.get("structure_type", "paragraph"),
                               has_table=bool(chunk_metadata.get("has_table", False)),
                               has_list=bool(chunk_metadata.get("has_list", False)),
                               section_title=chunk_metadata.get("section_title", ""),
                               page_number=int(chunk_metadata.get("page_number", 0)),
                               embedding=embedding)

                    # Document -> Chunk ê´€ê³„ ìƒì„±
                    session.run("""
                        MATCH (d:Document {doc_id: $doc_id})
                        MATCH (c:Chunk {chunk_id: $chunk_id})
                        MERGE (d)-[:HAS_CHUNK]->(c)
                    """, doc_id=doc_id, chunk_id=chunk_id)

                    chunk_ids.append(chunk_id)

                # 4. ìˆœì°¨ ì²­í¬ ê°„ NEXT_CHUNK ê´€ê³„ ìƒì„±
                if len(chunk_ids) > 1:
                    session.run("""
                        MATCH (d:Document {doc_id: $doc_id})-[:HAS_CHUNK]->(c:Chunk)
                        WITH c ORDER BY c.chunk_index
                        WITH collect(c) AS chunks
                        UNWIND range(0, size(chunks)-2) AS i
                        WITH chunks[i] AS curr, chunks[i+1] AS next
                        MERGE (curr)-[:NEXT_CHUNK]->(next)
                    """, doc_id=doc_id)

            logger.info(f"âœ… Added document {doc_id} with {len(chunks)} chunks to Neo4j")
            return True

        except Exception as e:
            logger.error(f"Failed to add document to Neo4j: {e}", exc_info=True)
            return False

    def search(
        self,
        query: str,
        top_k: int = 3,
        filter_metadata: Optional[Dict] = None,
        use_graph_expansion: bool = True
    ) -> List[Dict]:
        """
        Neo4j ê¸°ë°˜ GraphRAG ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
            filter_metadata: ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {"category": "ë³´í—˜"})
            use_graph_expansion: ê·¸ë˜í”„ í™•ì¥ ì‚¬ìš© ì—¬ë¶€ (ìˆœì°¨ ì»¨í…ìŠ¤íŠ¸)
        """
        tools_mode = os.getenv("TOOLS_RETRIEVER_MODE", "auto").lower()
        if tools_mode in {"graph", "vector"}:
            forced_graph = tools_mode == "graph"
            logger.info("ToolsRetriever forced mode=%s", tools_mode)
            return self._search_impl(
                query=query,
                top_k=top_k,
                filter_metadata=filter_metadata,
                use_graph_expansion=forced_graph,
            )

        if not use_graph_expansion:
            logger.info("ToolsRetriever bypassed (use_graph_expansion=False)")
            return self._search_impl(
                query=query,
                top_k=top_k,
                filter_metadata=filter_metadata,
                use_graph_expansion=False,
            )

        return self.tools_retriever.retrieve(
            query=query,
            top_k=top_k,
            filter_metadata=filter_metadata,
        )

    def _search_impl(
        self,
        query: str,
        top_k: int = 3,
        filter_metadata: Optional[Dict] = None,
        use_graph_expansion: bool = True,
    ) -> List[Dict]:
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_with_prefix = f"query: {query}"
            logger.info(f"ğŸ” _search_impl called: query='{query}', top_k={top_k}, use_graph_expansion={use_graph_expansion}")
            query_embedding = self.embedding_model.encode(query_with_prefix).tolist()
            logger.info(f"  - Generated embedding vector of length: {len(query_embedding)}")

            with self.driver.session() as session:
                if use_graph_expansion:
                    # GraphRAG: ë²¡í„° ê²€ìƒ‰ + ìˆœì°¨ ì»¨í…ìŠ¤íŠ¸ í™•ì¥
                    cypher_query = """
                        CALL db.index.vector.queryNodes('chunk_embeddings', $top_k, $embedding)
                        YIELD node AS c, score

                        // ìˆœì°¨ ì»¨í…ìŠ¤íŠ¸ í™•ì¥
                        OPTIONAL MATCH (prev:Chunk)-[:NEXT_CHUNK]->(c)
                        OPTIONAL MATCH (c)-[:NEXT_CHUNK]->(next:Chunk)

                        // ë¬¸ì„œ ë° ì¹´í…Œê³ ë¦¬ ì •ë³´
                        MATCH (d:Document)-[:HAS_CHUNK]->(c)
                        OPTIONAL MATCH (d)-[:BELONGS_TO]->(cat:Category)

                        // ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ ìµœì‹  ë¬¸ì„œ
                        OPTIONAL MATCH (cat)<-[:BELONGS_TO]-(related:Document)
                        WHERE related <> d

                        RETURN
                            c.chunk_id AS chunk_id,
                            c.content AS content,
                            c.title AS title,
                            c.chunk_index AS chunk_index,
                            c.structure_type AS structure_type,
                            c.has_table AS has_table,
                            c.has_list AS has_list,
                            score,
                            prev.content AS prev_context,
                            next.content AS next_context,
                            d.doc_id AS doc_id,
                            d.title AS doc_title,
                            d.file_path AS file_path,
                            cat.name AS category,
                            collect(DISTINCT {
                                doc_id: related.doc_id,
                                title: related.title,
                                created_at: related.created_at
                            })[0..3] AS related_docs
                        ORDER BY score DESC
                        LIMIT $top_k
                    """
                else:
                    # ë‹¨ìˆœ ë²¡í„° ê²€ìƒ‰
                    cypher_query = """
                        CALL db.index.vector.queryNodes('chunk_embeddings', $top_k, $embedding)
                        YIELD node AS c, score

                        MATCH (d:Document)-[:HAS_CHUNK]->(c)
                        OPTIONAL MATCH (d)-[:BELONGS_TO]->(cat:Category)

                        RETURN
                            c.chunk_id AS chunk_id,
                            c.content AS content,
                            c.title AS title,
                            c.structure_type AS structure_type,
                            score,
                            d.doc_id AS doc_id,
                            d.title AS doc_title,
                            cat.name AS category
                        ORDER BY score DESC
                        LIMIT $top_k
                    """

                logger.info(f"  - Executing {'graph expansion' if use_graph_expansion else 'simple vector'} search with top_k={top_k * 2}")
                result = session.run(cypher_query, embedding=query_embedding, top_k=top_k * 2)

                # ê²°ê³¼ í¬ë§·íŒ…
                results = []
                record_count = 0
                for record in result:
                    record_count += 1
                    item = {
                        "chunk_id": record.get("chunk_id"),
                        "content": record.get("content"),
                        "metadata": {
                            "title": record.get("title"),
                            "doc_id": record.get("doc_id"),
                            "doc_title": record.get("doc_title"),
                            "chunk_index": record.get("chunk_index"),
                            "structure_type": record.get("structure_type"),
                            "has_table": record.get("has_table"),
                            "has_list": record.get("has_list"),
                            "category": record.get("category"),
                            "file_path": record.get("file_path"),
                        },
                        "distance": 1 - record.get("score", 0),  # ìœ ì‚¬ë„ -> ê±°ë¦¬ ë³€í™˜
                        "relevance_score": record.get("score", 0),
                    }

                    # ìˆœì°¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                    if use_graph_expansion:
                        prev_context = record.get("prev_context")
                        next_context = record.get("next_context")
                        related_docs = record.get("related_docs", [])

                        if prev_context or next_context or related_docs:
                            item["context"] = {}
                            if prev_context:
                                item["context"]["prev"] = prev_context
                            if next_context:
                                item["context"]["next"] = next_context
                            if related_docs:
                                item["context"]["related_docs"] = [
                                    doc for doc in related_docs if doc.get("doc_id")
                                ]

                    results.append(item)

                # ì¹´í…Œê³ ë¦¬ í•„í„° ì ìš© (ë©”íƒ€ë°ì´í„° í•„í„°)
                if filter_metadata and "category" in filter_metadata:
                    filter_category = filter_metadata["category"]
                    results = [r for r in results if r["metadata"].get("category") == filter_category]

                logger.info(f"  - Retrieved {record_count} raw results from Neo4j")

                # top_k ê°œë§Œ ë°˜í™˜
                results = results[:top_k]

                logger.info(f"âœ… Found {len(results)} results for query: {query[:50]}...")
                if len(results) > 0:
                    logger.info(f"  - Top result score: {results[0].get('relevance_score', 0):.4f}")
                    logger.info(f"  - Top result preview: {results[0].get('content', '')[:100]}...")
                else:
                    logger.warning("  âš ï¸ No results found! Checking if vector index exists...")

                return results

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return []

    def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ (Document ë° ì—°ê²°ëœ Chunkë“¤ ì‚­ì œ)"""
        try:
            with self.driver.session() as session:
                result = session.run("""
                    MATCH (d:Document {doc_id: $doc_id})
                    OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)
                    DETACH DELETE d, c
                    RETURN count(d) AS deleted_count
                """, doc_id=doc_id)

                record = result.single()
                deleted_count = record["deleted_count"] if record else 0

                if deleted_count > 0:
                    logger.info(f"âœ… Deleted document {doc_id}")
                    return True
                else:
                    logger.warning(f"Document {doc_id} not found")
                    return False

        except Exception as e:
            logger.error(f"Failed to delete document {doc_id}: {e}", exc_info=True)
            return False

    def get_collection_stats(self) -> Dict:
        """Neo4j ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        try:
            with self.driver.session() as session:
                # ë¬¸ì„œ ë° ì²­í¬ ìˆ˜ ì¡°íšŒ
                result = session.run("""
                    MATCH (d:Document)
                    OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)
                    RETURN count(DISTINCT d) AS doc_count, count(c) AS chunk_count
                """)
                record = result.single()

                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                category_stats = session.run("""
                    MATCH (d:Document)-[:BELONGS_TO]->(cat:Category)
                    RETURN cat.name AS category, count(d) AS doc_count
                    ORDER BY doc_count DESC
                """).data()

                return {
                    "vector_db": "neo4j",
                    "graph_db": "neo4j",
                    "status": "available",
                    "total_documents": record["doc_count"] if record else 0,
                    "total_chunks": record["chunk_count"] if record else 0,
                    "vector_size": self.embedding_dim,
                    "categories": category_stats,
                    "graph_rag_enabled": True,
                }

        except Exception as e:
            logger.error(f"Failed to get stats: {e}", exc_info=True)
            return {
                "vector_db": "neo4j",
                "status": "error",
                "error": str(e),
            }

    def close(self):
        """Neo4j ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.close()
            logger.info("Neo4j driver closed")
