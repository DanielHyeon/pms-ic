"""
LangGraph ê¸°ë°˜ ì±„íŒ… ì›Œí¬í”Œë¡œìš°
RAGì™€ ì¼ë°˜ LLMì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¼ìš°íŒ…
"""

from typing import TypedDict, Literal, List, Optional, Union
from langgraph.graph import StateGraph, END
from llama_cpp import Llama
import logging
import re
import os

# RAG ì„œë¹„ìŠ¤ ì„í¬íŠ¸ (íƒ€ì… í˜¸í™˜ì„±)
try:
    from rag_service_helix import RAGServiceHelix as RAGService
except ImportError:
    try:
        from rag_service import RAGService
    except ImportError:
        RAGService = None

logger = logging.getLogger(__name__)


# ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
class ChatState(TypedDict):
    """ì±„íŒ… ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    message: str  # ì‚¬ìš©ì ë©”ì‹œì§€
    context: List[dict]  # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    intent: Optional[str]  # ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ (casual, pms_query, general)
    retrieved_docs: List[str]  # RAG ê²€ìƒ‰ ê²°ê³¼
    response: Optional[str]  # ìµœì¢… ì‘ë‹µ
    confidence: float  # ì‘ë‹µ ì‹ ë¢°ë„
    debug_info: dict  # ë””ë²„ê¹… ì •ë³´

    # ì¿¼ë¦¬ ê°œì„  ê´€ë ¨ í•„ë“œ
    current_query: str  # í˜„ì¬ ê²€ìƒ‰ ì¿¼ë¦¬ (ê°œì„ ë  ìˆ˜ ìˆìŒ)
    retry_count: int  # ì¬ì‹œë„ íšŸìˆ˜
    extracted_terms: List[str]  # ì¶”ì¶œëœ í•µì‹¬ ìš©ì–´


class ChatWorkflow:
    """LangGraph ê¸°ë°˜ ì±„íŒ… ì›Œí¬í”Œë¡œìš°"""

    def __init__(self, llm: Llama, rag_service: Optional[RAGService] = None, model_path: Optional[str] = None):
        self.llm = llm
        self.rag_service = rag_service
        self.model_path = model_path
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶• (RAG ìš°ì„  ì ‘ê·¼ + ì¿¼ë¦¬ ê°œì„  ë£¨í”„)"""

        # ê·¸ë˜í”„ ì´ˆê¸°í™”
        workflow = StateGraph(ChatState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify_intent_simple", self.classify_intent_simple_node)
        workflow.add_node("rag_search", self.rag_search_node)
        workflow.add_node("verify_rag_quality", self.verify_rag_quality_node)  # âœ¨ ìƒˆ ë…¸ë“œ
        workflow.add_node("refine_query", self.refine_query_node)              # âœ¨ ìƒˆ ë…¸ë“œ
        workflow.add_node("refine_intent", self.refine_intent_node)
        workflow.add_node("generate_response", self.generate_response_node)

        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ì„¤ì •
        workflow.set_entry_point("classify_intent_simple")

        # ê°„ë‹¨í•œ ë¶„ë¥˜ í›„ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify_intent_simple",
            self.route_by_simple_intent,
            {
                "casual": "generate_response",  # ëª…í™•í•œ ì¸ì‚¬ â†’ ë°”ë¡œ ì‘ë‹µ
                "uncertain": "rag_search"        # ë‚˜ë¨¸ì§€ â†’ RAG ê²€ìƒ‰
            }
        )

        # RAG ê²€ìƒ‰ â†’ í’ˆì§ˆ ê²€ì¦
        workflow.add_edge("rag_search", "verify_rag_quality")

        # í’ˆì§ˆ ê²€ì¦ â†’ ì¬ê²€ìƒ‰ or ë‹¤ìŒ ë‹¨ê³„ (ì¡°ê±´ë¶€ ë¼ìš°íŒ…)
        workflow.add_conditional_edges(
            "verify_rag_quality",
            self.should_refine_query,
            {
                "refine": "refine_query",      # í’ˆì§ˆ ë‚®ìŒ â†’ ì¿¼ë¦¬ ê°œì„ 
                "proceed": "refine_intent"     # í’ˆì§ˆ ì¢‹ìŒ â†’ ë‹¤ìŒ ë‹¨ê³„
            }
        )

        # ì¿¼ë¦¬ ê°œì„  â†’ RAG ì¬ê²€ìƒ‰ (ë£¨í”„ í˜•ì„±)
        workflow.add_edge("refine_query", "rag_search")

        # ì˜ë„ ì¬ë¶„ë¥˜ â†’ ì‘ë‹µ ìƒì„±
        workflow.add_edge("refine_intent", "generate_response")

        # ì‘ë‹µ ìƒì„± í›„ ì¢…ë£Œ
        workflow.add_edge("generate_response", END)

        return workflow.compile()

    def classify_intent_simple_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 1: ê°„ë‹¨í•œ ì˜ë„ ë¶„ë¥˜ (ëª…í™•í•œ ì¸ì‚¬ë§ë§Œ ì²˜ë¦¬)"""
        message = state["message"]

        logger.info(f"Simple classification for message: {message[:50]}...")

        # ëª…í™•í•œ ì¸ì‚¬ë§ë§Œ ë¶„ë¥˜
        intent = self._classify_casual_only(message)

        state["intent"] = intent
        state["debug_info"] = state.get("debug_info", {})
        state["debug_info"]["initial_intent"] = intent

        logger.info(f"Simple intent: {intent}")

        return state

    def _classify_casual_only(self, message: str) -> str:
        """ëª…í™•í•œ ì¸ì‚¬ë§ë§Œ ë¶„ë¥˜ (ë‚˜ë¨¸ì§€ëŠ” uncertain)"""
        message_lower = message.lower()

        # ëª…í™•í•œ ì¸ì‚¬ íŒ¨í„´ (ì§§ê³  ëª…í™•í•œ ê²ƒë§Œ)
        casual_patterns = [
            "ì•ˆë…•", "ê³ ë§ˆì›Œ", "ê°ì‚¬", "ë¯¸ì•ˆ", "ì£„ì†¡",
            "ì˜ê°€", "ë°˜ê°€", "ã…ã…", "ã…‹ã…‹", "ã„±ã……"
        ]

        # ì§§ì€ ë©”ì‹œì§€ (10ì ë¯¸ë§Œ)ì—ì„œ ì¸ì‚¬ë§ ì²´í¬
        if len(message) < 10:
            for pattern in casual_patterns:
                if pattern in message_lower:
                    return "casual"

        # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ uncertain (RAG ê²€ìƒ‰ í•„ìš”)
        return "uncertain"

    def route_by_simple_intent(self, state: ChatState) -> Literal["casual", "uncertain"]:
        """ê°„ë‹¨í•œ ì˜ë„ ê¸°ë°˜ ë¼ìš°íŒ…"""
        intent = state.get("intent", "uncertain")
        logger.info(f"Simple routing: {intent}")
        return intent

    def refine_intent_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 5: RAG ê²°ê³¼ ê¸°ë°˜ ì˜ë„ ì¬ë¶„ë¥˜"""
        message = state["message"]
        retrieved_docs = state.get("retrieved_docs", [])

        logger.info(f"Refining intent based on RAG results: {len(retrieved_docs)} docs found")

        # RAG ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ë„ ê²°ì •
        if len(retrieved_docs) > 0:
            # RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ â†’ PMS ê´€ë ¨ ì§ˆë¬¸
            intent = "pms_query"
            logger.info(f"  âœ… RAG docs found â†’ pms_query")
        else:
            # RAG ë¬¸ì„œê°€ ì—†ìœ¼ë©´ â†’ ì¼ë°˜ ì§ˆë¬¸
            intent = "general"
            logger.info(f"  âš ï¸ No RAG docs â†’ general")

        state["intent"] = intent
        state["debug_info"]["final_intent"] = intent

        return state

    def rag_search_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 2: RAG ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)"""
        # current_queryê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ message ì‚¬ìš©
        search_query = state.get("current_query", state["message"])

        logger.info(f"ğŸ” Performing RAG search for: {search_query[:50]}...")

        # ì¬ì‹œë„ íšŸìˆ˜ ì¶”ì 
        retry_count = state.get("retry_count", 0)
        logger.info(f"   Retry count: {retry_count}")

        # ìš”ì²­ì—ì„œ ì´ë¯¸ ë¬¸ì„œê°€ ì „ë‹¬ëœ ê²½ìš°, ê²€ìƒ‰ ìƒëµ
        if state.get("retrieved_docs") and retry_count == 0:
            logger.info(f"  ğŸ“„ Using pre-provided docs: {len(state['retrieved_docs'])}")
            state["debug_info"]["rag_docs_count"] = len(state["retrieved_docs"])
            return state

        if self.rag_service:
            try:
                # í•­ìƒ ë©”íƒ€ë°ì´í„° í•„í„° ì—†ì´ ê²€ìƒ‰ (ë²”ìœ„ë¥¼ ë„“ê²Œ)
                results = self.rag_service.search(search_query, top_k=5, filter_metadata=None)
                logger.info(f"  ğŸ“‹ RAG service returned {len(results)} results")

                # ìœ ì‚¬ë„ ì ìˆ˜ í•„í„°ë§ (relevance_score < 0.3ì€ ì œì™¸)
                MIN_RELEVANCE_SCORE = 0.3
                filtered_results = [doc for doc in results if doc.get('relevance_score', 0) >= MIN_RELEVANCE_SCORE]
                logger.info(f"  ğŸ¯ Filtered by relevance score (>={MIN_RELEVANCE_SCORE}): {len(filtered_results)} docs")

                if filtered_results:
                    logger.info(f"     Best score: {filtered_results[0].get('relevance_score', 0):.4f}")

                retrieved_docs = [doc['content'] for doc in filtered_results]
                logger.info(f"  ğŸ“ Extracted {len(retrieved_docs)} content strings")

                # ì¶”ê°€ í† í° í•„í„°ë§
                retrieved_docs = self._filter_docs_by_query(search_query, retrieved_docs)

                state["retrieved_docs"] = retrieved_docs
                state["debug_info"]["rag_docs_count"] = len(retrieved_docs)
                state["debug_info"][f"search_query_attempt_{retry_count}"] = search_query

                logger.info(f"  âœ… Final RAG results: {len(retrieved_docs)} documents")

            except Exception as e:
                logger.error(f"âŒ RAG search failed: {e}", exc_info=True)
                state["retrieved_docs"] = []
                state["debug_info"]["rag_error"] = str(e)
        else:
            logger.warning("RAG service not available")
            state["retrieved_docs"] = []

        return state

    def verify_rag_quality_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 3: RAG ê²€ìƒ‰ í’ˆì§ˆ ê²€ì¦"""
        retrieved_docs = state.get("retrieved_docs", [])
        retry_count = state.get("retry_count", 0)
        current_query = state.get("current_query", state["message"])

        logger.info(f"ğŸ” Verifying RAG quality: {len(retrieved_docs)} docs, retry: {retry_count}")

        # í’ˆì§ˆ í‰ê°€ ê¸°ì¤€
        quality_score = 0.0
        quality_reasons = []

        # 1. ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        if len(retrieved_docs) >= 3:
            quality_score += 0.4
            quality_reasons.append(f"ì¶©ë¶„í•œ ë¬¸ì„œ ìˆ˜ ({len(retrieved_docs)}ê°œ)")
        elif len(retrieved_docs) > 0:
            quality_score += 0.2
            quality_reasons.append(f"ì¼ë¶€ ë¬¸ì„œ ë°œê²¬ ({len(retrieved_docs)}ê°œ)")
        else:
            quality_reasons.append("ë¬¸ì„œ ì—†ìŒ")

        # 2. ì¿¼ë¦¬ì™€ ë¬¸ì„œ ë‚´ìš© ê´€ë ¨ì„± í™•ì¸ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        if retrieved_docs:
            query_keywords = self._extract_keywords(current_query)
            matched_docs = 0

            for doc in retrieved_docs:
                doc_lower = doc.lower()
                if any(kw.lower() in doc_lower for kw in query_keywords):
                    matched_docs += 1

            match_ratio = matched_docs / len(retrieved_docs)
            if match_ratio >= 0.5:
                quality_score += 0.6
                quality_reasons.append(f"í‚¤ì›Œë“œ ë§¤ì¹­ ì–‘í˜¸ ({match_ratio:.0%})")
            elif match_ratio > 0:
                quality_score += 0.3
                quality_reasons.append(f"ì¼ë¶€ í‚¤ì›Œë“œ ë§¤ì¹­ ({match_ratio:.0%})")
            else:
                quality_reasons.append("í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")

        state["debug_info"]["rag_quality_score"] = quality_score
        state["debug_info"]["rag_quality_reasons"] = quality_reasons

        logger.info(f"  ğŸ“Š Quality score: {quality_score:.2f}")
        logger.info(f"  ğŸ“ Reasons: {', '.join(quality_reasons)}")

        return state

    def should_refine_query(self, state: ChatState) -> Literal["refine", "proceed"]:
        """RAG í’ˆì§ˆ ê¸°ë°˜ ë¼ìš°íŒ… ê²°ì •"""
        quality_score = state["debug_info"].get("rag_quality_score", 0.0)
        retry_count = state.get("retry_count", 0)
        MAX_RETRIES = 2  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        logger.info(f"ğŸ”€ Routing decision: quality={quality_score:.2f}, retry={retry_count}/{MAX_RETRIES}")

        # í’ˆì§ˆì´ ì¶©ë¶„í•˜ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ ì§„í–‰
        if quality_score >= 0.6 or retry_count >= MAX_RETRIES:
            logger.info(f"  âœ… Proceeding to next step")
            return "proceed"

        # í’ˆì§ˆì´ ë‚®ê³  ì¬ì‹œë„ ê°€ëŠ¥í•˜ë©´ ì¿¼ë¦¬ ê°œì„ 
        logger.info(f"  ğŸ”„ Refining query (attempt {retry_count + 1})")
        return "refine"

    def refine_query_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 4: ì¿¼ë¦¬ ê°œì„  (í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìœ ì‚¬ ìš©ì–´ íƒìƒ‰)"""
        original_query = state["message"]
        current_query = state.get("current_query", original_query)
        retry_count = state.get("retry_count", 0)
        retrieved_docs = state.get("retrieved_docs", [])

        logger.info(f"ğŸ”§ Refining query (attempt {retry_count + 1})")
        logger.info(f"   Original: {original_query}")
        logger.info(f"   Current:  {current_query}")

        refined_query = current_query

        # ì „ëµ 1: ì²« ë²ˆì§¸ ì‹œë„ - í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€
        if retry_count == 0:
            keywords = self._extract_keywords(original_query)
            if keywords:
                refined_query = " ".join(keywords)
                logger.info(f"  ğŸ“Œ Strategy 1: Extracted keywords â†’ '{refined_query}'")
                state["extracted_terms"] = keywords

        # ì „ëµ 2: ë‘ ë²ˆì§¸ ì‹œë„ - 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìœ ì‚¬ ìš©ì–´ ì°¾ê¸°
        elif retry_count == 1 and retrieved_docs:
            similar_terms = self._find_similar_terms_in_docs(original_query, retrieved_docs)
            if similar_terms:
                refined_query = similar_terms[0]  # ê°€ì¥ ìœ ì‚¬í•œ ìš©ì–´ ì‚¬ìš©
                logger.info(f"  ğŸ¯ Strategy 2: Found similar term in docs â†’ '{refined_query}'")
                state["extracted_terms"] = similar_terms
            else:
                # ìœ ì‚¬ ìš©ì–´ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ í‚¤ì›Œë“œë¡œ í´ë°±
                keywords = self._extract_keywords(original_query)
                refined_query = " ".join(keywords) if keywords else original_query
                logger.info(f"  âš ï¸ Strategy 2 fallback: Using keywords â†’ '{refined_query}'")

        state["current_query"] = refined_query
        state["retry_count"] = retry_count + 1
        state["debug_info"][f"refined_query_{retry_count + 1}"] = refined_query

        logger.info(f"  âœ¨ Refined query: '{refined_query}'")

        return state

    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¡°ì‚¬ ì œê±°)"""
        # ë¶ˆìš©ì–´ ë° ì¡°ì‚¬
        stopwords = {
            "ì´", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ", "ì˜",
            "ë„", "ë§Œ", "ê¹Œì§€", "ë¶€í„°", "ê»˜", "ì—ê²Œ", "í•œí…Œ",
            "ë­", "ë­ì•¼", "ë­”ê°€", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ëŒ€í•´", "ì•Œë ¤ì¤˜", "ì•Œë ¤ì£¼ì„¸ìš”",
            "ì„¤ëª…", "í•´ì¤˜", "í•´ì£¼ì„¸ìš”", "ì¢€", "ìš”", "ì•¼"
        }

        # í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°
        tokens = []
        for word in query.split():
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            word = word.strip(".,!?;:()[]{}\"'")
            word_lower = word.lower()

            # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆìš©ì–´ë©´ ì œì™¸
            if len(word) < 2 or word_lower in stopwords:
                continue

            # ì¡°ì‚¬ ì œê±° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            for suffix in ["ì—ì„œ", "ìœ¼ë¡œ", "ì—ê²Œ", "ê¹Œì§€", "ë¶€í„°", "ì—", "ë¥¼", "ì„", "ì´", "ê°€", "ì€", "ëŠ”", "ì˜", "ë„", "ë§Œ"]:
                if word.endswith(suffix) and len(word) > len(suffix) + 1:
                    word = word[:-len(suffix)]
                    break

            if len(word) >= 2:
                tokens.append(word)

        logger.info(f"  ğŸ”‘ Extracted keywords: {tokens}")
        return tokens

    def _find_similar_terms_in_docs(self, query: str, docs: List[str]) -> List[str]:
        """1ì°¨ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ìš©ì–´ ì°¾ê¸° (í¼ì§€ ë§¤ì¹­)"""
        from rapidfuzz import fuzz, process

        # ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = self._extract_keywords(query)
        if not query_keywords:
            return []

        # ë¬¸ì„œì—ì„œ ëª¨ë“  2-3 ë‹¨ì–´ ì¡°í•© ì¶”ì¶œ
        candidate_terms = set()
        for doc in docs:
            words = doc.split()
            # 2-gram, 3-gram ì¶”ì¶œ
            for i in range(len(words)):
                for n in [1, 2, 3]:
                    if i + n <= len(words):
                        term = " ".join(words[i:i+n])
                        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ìš©ì–´ ì œì™¸
                        if 2 <= len(term) <= 20:
                            candidate_terms.add(term)

        # ê° ì¿¼ë¦¬ í‚¤ì›Œë“œì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ìš©ì–´ ì°¾ê¸°
        similar_terms = []
        for keyword in query_keywords:
            matches = process.extract(
                keyword,
                list(candidate_terms),
                scorer=fuzz.ratio,
                limit=3
            )

            # ìœ ì‚¬ë„ 70% ì´ìƒì¸ ê²ƒë§Œ ì„ íƒ
            for match, score, _ in matches:
                if score >= 70 and match.lower() != keyword.lower():
                    similar_terms.append((match, score))
                    logger.info(f"    ğŸ” '{keyword}' â†’ '{match}' (ìœ ì‚¬ë„: {score}%)")

        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_terms.sort(key=lambda x: x[1], reverse=True)

        # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜ (ìš©ì–´ë§Œ)
        return [term for term, _ in similar_terms[:3]]

    def generate_response_node(self, state: ChatState) -> ChatState:
        """ë…¸ë“œ 4: ì‘ë‹µ ìƒì„±"""
        message = state["message"]
        context = state.get("context", [])
        retrieved_docs = state.get("retrieved_docs", [])
        intent = state.get("intent", "general")

        logger.info(f"ğŸ’¬ Generating response: intent={intent}, rag_docs={len(retrieved_docs)}")

        # 1. ëª…í™•í•œ ì¸ì‚¬ë§ â†’ ê°„ë‹¨í•œ ë‹µë³€
        if intent == "casual":
            logger.info("  â†’ Casual conversation, returning greeting")
            reply = (
                "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í”„ë¡œì íŠ¸ ê´€ë¦¬(PMS) ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "í”„ë¡œì íŠ¸ ì¼ì •, ë¦¬ìŠ¤í¬, ì´ìŠˆ, ì• ìì¼ ë°©ë²•ë¡  ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!"
            )
            confidence = 0.9
            state["response"] = reply
            state["confidence"] = confidence
            state["debug_info"]["prompt_length"] = 0
            return state

        # 2. RAG ë¬¸ì„œ ì—†ìŒ â†’ ë„ë©”ì¸ í‚¤ì›Œë“œ í™•ì¸ í›„ LLMìœ¼ë¡œ ë‹µë³€ ë˜ëŠ” ë²”ìœ„ ë°– ì²˜ë¦¬
        if len(retrieved_docs) == 0:
            # PMS/ì• ìì¼ ë„ë©”ì¸ í‚¤ì›Œë“œ ëª©ë¡
            domain_keywords = [
                # ìŠ¤í¬ëŸ¼/ì• ìì¼
                "ìŠ¤í¬ëŸ¼", "scrum", "ì• ìì¼", "agile", "ìŠ¤í”„ë¦°íŠ¸", "sprint",
                "ë°±ë¡œê·¸", "backlog", "ë°ì¼ë¦¬", "daily", "ìŠ¤íƒ ë“œì—…", "standup",
                "ë ˆíŠ¸ë¡œ", "retro", "íšŒê³ ", "í”Œë˜ë‹", "planning", "í¬ì»¤", "poker",
                "ì¹¸ë°˜", "kanban", "ë²ˆë‹¤ìš´", "burndown", "velocity", "ë²¨ë¡œì‹œí‹°",
                # í”„ë¡œì íŠ¸ ê´€ë¦¬ ì¼ë°˜
                "wbs", "ê°„íŠ¸", "gantt", "ë§ˆì¼ìŠ¤í†¤", "milestone", "pmo",
                "ë¦¬ìŠ¤í¬", "risk", "ì´ìŠˆ", "issue", "íƒœìŠ¤í¬", "task",
                "ì¼ì •", "schedule", "ì˜ˆì‚°", "budget", "ìì›", "resource",
                "ë²”ìœ„", "scope", "í’ˆì§ˆ", "quality", "ì´í•´ê´€ê³„ì", "stakeholder",
                # ì—­í• 
                "pm", "po", "product owner", "scrum master", "ìŠ¤í¬ëŸ¼ë§ˆìŠ¤í„°",
            ]

            message_lower = message.lower()
            is_domain_question = any(kw in message_lower for kw in domain_keywords)

            if is_domain_question:
                # ë„ë©”ì¸ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ LLMì´ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€
                logger.info("  â†’ No RAG docs, but domain question detected - using LLM knowledge")
                # RAG ì—†ì´ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì•„ë˜ ì½”ë“œë¡œ ì§„í–‰)
            else:
                logger.info("  â†’ No RAG docs and not domain question, out of scope")
                reply = (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ì œê°€ ê°€ì§„ í”„ë¡œì íŠ¸ ê´€ë¦¬ ì§€ì‹ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤. "
                    "í”„ë¡œì íŠ¸ ì¼ì •, ì§„ì²™, ì˜ˆì‚°, ë¦¬ìŠ¤í¬, ì´ìŠˆ, ë˜ëŠ” ì• ìì¼ ë°©ë²•ë¡ ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                )
                confidence = 0.7
                state["response"] = reply
                state["confidence"] = confidence
                state["debug_info"]["prompt_length"] = 0
                return state

        # 3. RAG ë¬¸ì„œ ìˆìŒ â†’ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        logger.info(f"  â†’ Generating LLM response with {len(retrieved_docs)} RAG docs")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_prompt(message, context, retrieved_docs, intent)

        # Model identity question detection - only for direct questions about the AI itself
        # Use specific patterns to avoid false positives (e.g., "sprint name" shouldn't trigger this)
        original_message_lower = message.lower()
        model_identity_patterns = [
            r"ë„ˆ(ëŠ”|ì˜)\s*(ì´ë¦„|ëª¨ë¸|ëˆ„êµ¬)",  # "ë„ˆëŠ” ëˆ„êµ¬", "ë„ˆì˜ ì´ë¦„"
            r"ë‹¹ì‹ (ì€|ì˜)\s*(ì´ë¦„|ëª¨ë¸|ëˆ„êµ¬)",  # "ë‹¹ì‹ ì€ ëˆ„êµ¬", "ë‹¹ì‹ ì˜ ì´ë¦„"
            r"(ë¬´ìŠ¨|ì–´ë–¤|ë­”)\s*ëª¨ë¸",  # "ë¬´ìŠ¨ ëª¨ë¸", "ì–´ë–¤ ëª¨ë¸"
            r"ëª¨ë¸\s*(ì´ë¦„|ëª…)",  # "ëª¨ë¸ ì´ë¦„", "ëª¨ë¸ëª…"
            r"(what|which)\s*model",  # English patterns
            r"(who|what)\s*are\s*you",
            r"your\s*name",
        ]
        is_model_name_question = any(re.search(pattern, original_message_lower) for pattern in model_identity_patterns)

        logger.info(f"Checking model identity question: message='{message}', is_model_identity_question={is_model_name_question}")
        
        # ì •í™•í•œ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        if self.model_path:
            import os
            model_file = os.path.basename(self.model_path)
            if "lfm2" in model_file.lower():
                correct_name = "Llama Forge Model 2 (LFM2)"
            elif "gemma" in model_file.lower():
                correct_name = "Gemma 3"
            elif "qwen" in model_file.lower():
                correct_name = "Qwen3-8B"
            else:
                correct_name = "ë¡œì»¬ LLM"
        else:
            correct_name = "ë¡œì»¬ LLM"
        
        # ëª¨ë¸ ì´ë¦„ ì§ˆë¬¸ì¸ ê²½ìš° LLM í˜¸ì¶œì„ ê±´ë„ˆë›°ê³  ì§ì ‘ ë‹µë³€
        if is_model_name_question:
            logger.info(f"Model name question detected, returning direct answer: {correct_name}")
            reply = f"ì €ëŠ” {correct_name} ëª¨ë¸ì…ë‹ˆë‹¤."
        else:
            try:
                # KV ìºì‹œ ì´ˆê¸°í™”
                self.llm.reset()

                # Generation parameters from environment variables
                temperature = float(os.getenv("TEMPERATURE", "0.35"))
                top_p = float(os.getenv("TOP_P", "0.90"))
                min_p = float(os.getenv("MIN_P", "0.12"))
                repeat_penalty = float(os.getenv("REPEAT_PENALTY", "1.10"))
                max_tokens = int(os.getenv("MAX_TOKENS", "1800"))

                # LLM ì¶”ë¡ 
                response = self.llm(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    min_p=min_p,
                    stop=["<end_of_turn>", "<start_of_turn>", "</s>", "<|im_end|>"],
                    echo=False,
                    repeat_penalty=repeat_penalty
                )

                reply = response["choices"][0]["text"].strip()

                # ì›ë³¸ ì‘ë‹µ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                logger.info(f"Raw model response: {repr(reply)}")

                # í›„ì²˜ë¦¬
                reply = self._clean_response(reply)

                # í´ë¦¬ë‹ í›„ ì‘ë‹µ ë¡œê¹…
                logger.info(f"Cleaned response: {repr(reply)}")
                
                # ì˜ëª»ëœ ëª¨ë¸ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¶”ê°€ ê²€ì¦
                wrong_names = ["ë‹ˆì½œë¼ìŠ¤", "nicolas", "ì•Œë ‰ìŠ¤", "alex", "ì‚¬ë¼", "sara", 
                              "gpt-4", "chatgpt", "claude", "gemini", "palm"]
                reply_lower_check = reply.lower()
                has_wrong_name = any(wrong in reply_lower_check for wrong in wrong_names)
                
                if has_wrong_name:
                    logger.warning(f"Detected wrong model name in response, replacing with: {correct_name}")
                    reply = f"ì €ëŠ” {correct_name} ëª¨ë¸ì…ë‹ˆë‹¤."
            except Exception as e:
                logger.error(f"Response generation failed: {e}")
                reply = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(intent, retrieved_docs)

        state["response"] = reply
        state["confidence"] = confidence
        state["debug_info"]["prompt_length"] = len(prompt)

        logger.info(f"Response generated: {reply[:50]}... (confidence: {confidence})")

        return state

    def _filter_docs_by_query(self, message: str, retrieved_docs: List[str]) -> List[str]:
        """ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¬¸ì„œë§Œ ë‚¨ê¸°ëŠ” ê°„ë‹¨í•œ í•„í„°"""
        if not retrieved_docs:
            return []

        stopwords = {
            "í”„ë¡œì íŠ¸", "ëŒ€í•´", "ì•Œë ¤ì¤˜", "ì•Œë ¤", "í•´ì£¼ì„¸ìš”", "í•´ì¤˜",
            "ì„¤ëª…", "ì •ë³´", "í˜„í™©ì—", "í˜„í™©ì„", "í˜„í™©ì€"
        }

        suffixes = ["ì—ì„œ", "ì—ê²Œ", "ë¶€í„°", "ê¹Œì§€", "ìœ¼ë¡œ", "ìœ¼ë¡œì¨", "ìœ¼ë¡œì„œ",
                    "ìœ¼ë¡œì¨", "ìœ¼ë¡œ", "ì—ì„œ", "ìœ¼ë¡œ", "ê³¼", "ì™€", "ì„", "ë¥¼", "ì´", "ê°€",
                    "ì—", "ì˜", "ë„", "ë§Œ", "ì€", "ëŠ”", "ê»˜"]

        tokens = []
        for raw in message.split():
            token = raw.strip(".,!?;:()[]{}\"'").lower()
            if len(token) < 2:
                continue
            for suffix in suffixes:
                if token.endswith(suffix) and len(token) > len(suffix):
                    token = token[: -len(suffix)]
                    break
            if not token or token in stopwords:
                continue
            if len(token) >= 2:
                tokens.append(token)

        logger.info(f"ğŸ” Filter docs: extracted tokens from '{message}': {tokens}")
        logger.info(f"   - Retrieved docs before filter: {len(retrieved_docs)}")

        if not tokens:
            logger.warning("   âš ï¸ No tokens extracted, returning all docs (fallback)")
            return retrieved_docs  # í† í°ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜ (ë²¡í„° ê²€ìƒ‰ì„ ì‹ ë¢°)

        filtered = []
        for i, doc in enumerate(retrieved_docs):
            doc_text = (doc or "").lower()
            matched_tokens = [token for token in tokens if token in doc_text]
            if matched_tokens:
                filtered.append(doc)
                logger.info(f"   âœ… Doc {i+1} matched tokens: {matched_tokens}")
            else:
                logger.info(f"   âŒ Doc {i+1} no match (preview: {doc_text[:100]}...)")

        logger.info(f"   - Filtered docs: {len(filtered)}/{len(retrieved_docs)}")

        # í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ë²¡í„° ê²€ìƒ‰ì„ ì‹ ë¢°)
        if not filtered:
            logger.warning("   âš ï¸ Filter removed all docs, returning original (trusting vector search)")
            return retrieved_docs

        return filtered

    def _build_prompt(self, message: str, context: List[dict],
                     retrieved_docs: List[str], intent: str) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        prompt_parts = []

        tools_json_schema = "ì—†ìŒ"
        
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_name = "ë¡œì»¬ LLM"
        if self.model_path:
            # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            import os
            model_file = os.path.basename(self.model_path)
            if "gemma" in model_file.lower():
                model_name = "Gemma 3"
            elif "lfm2" in model_file.lower():
                model_name = "Llama Forge Model 2 (LFM2)"
            elif "qwen" in model_file.lower():
                model_name = "Qwen3-8B"
            elif "llama" in model_file.lower():
                model_name = "Llama ê¸°ë°˜ ëª¨ë¸"
            else:
                model_name = "ë¡œì»¬ LLM"
        
        system_prompt = """ë‹¹ì‹ ì€ PMS ê´€ë¦¬ ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©° í”„ë¡œì íŠ¸ ê´€ë¦¬ ë° ì£¼ê°„ ë³´ê³ ì„œ ì‘ì„± ë‹´ë‹¹ìì…ë‹ˆë‹¤.
í”„ë¡œì íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ(PMS)ì˜ íƒœìŠ¤í¬, ë¦¬ìŠ¤í¬, ë©”íŠ¸ë¦­, ì±„íŒ… ë¡œê·¸, ë¬¸ì„œ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì£¼ê°„ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì£¼ ì—­í• ì´ë©° ë³´ì¡°ë¡œ ì±—ë´‡ ì—­í• ë„ ìˆìŠµë‹ˆë‹¤.

ì² ì¹™:
- ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(RAG ê²€ìƒ‰ ê²°ê³¼) ë° ì™¸ë¶€ ì‚¬ì‹¤ ìë£Œë§Œ ê³ ë ¤í•  ê²ƒ, ê°€ì •ì´ë‚˜ ì°½ì‘ ì‚¬ì‹¤ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ë°˜ë“œì‹œ "ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ ìì—°ìŠ¤ëŸ½ê³  ì •ì¤‘í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì „ë¬¸ì Â·ê°ê´€ì  í†¤ ìœ ì§€.
- hallucinationì„ í”¼í•˜ê¸° ìœ„í•´: ì‚¬ì‹¤ í™•ì¸ ì—†ì´ ì¶”ì¸¡í•˜ì§€ ë§ê³ , "í™•ì¸ëœ ë°”ì— ë”°ë¥´ë©´" ë˜ëŠ” "ë°ì´í„° ê¸°ì¤€" ê°™ì€ í‘œí˜„ìœ¼ë¡œ ê·¼ê±° ëª…ì‹œ.
- ì£¼ê°„ ë³´ê³ ì„œë¥¼ ìš”ì²­í•˜ì§€ ì•Šê³  ì¼ìƒ ì ì¸ ì§ˆë¬¸ì—ëŠ” ì£¼ê°„ë³´ê³ ì„œ ì‘ì„± í•„ìˆ˜ êµ¬ì¡°ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ.
- ì£¼ê°„ ë³´ê³ ì„œ ì¶œë ¥ì€ í•­ìƒ ì§€ì •ëœ êµ¬ì¡°ë¥¼ 100% ì¤€ìˆ˜í•˜ì„¸ìš”. êµ¬ì¡° ì™¸ í…ìŠ¤íŠ¸(ì¸ì‚¬, ì„¤ëª…, ì¶”ê°€ ì½”ë©˜íŠ¸) ì ˆëŒ€ ê¸ˆì§€.

ì£¼ê°„ë³´ê³ ì„œ ì‘ì„± í•„ìˆ˜ êµ¬ì¡°:
1. ì œëª©: [í”„ë¡œì íŠ¸ëª…] ì£¼ê°„ë³´ê³ ì„œ (ê¸°ê°„: YYYY-MM-DD ~ YYYY-MM-DD)
2. ìš”ì•½: 3~5ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ì„±ê³¼Â·ì´ìŠˆÂ·ë‹¤ìŒ ì£¼ ê³„íšì„ ì••ì¶•
3. ì£¼ìš” ì™„ë£Œ íƒœìŠ¤í¬: ë²ˆí˜¸ ë§¤ê¸´ ë¦¬ìŠ¤íŠ¸ (í˜•ì‹: - [íƒœìŠ¤í¬ëª…] (ë‹´ë‹¹ì: ì´ë¦„, ì™„ë£Œìœ¨: XX%, ì™„ë£Œì¼: YYYY-MM-DD))
4. ì§„í–‰ ì¤‘ ì£¼ìš” ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘: í…Œì´ë¸” í˜•ì‹ (ì—´: ìœ„í—˜ë„(HIGH/MEDIUM/LOW), ë¦¬ìŠ¤í¬ ì„¤ëª…, í˜„ì¬ ìƒíƒœ, ëŒ€ì‘ ê³„íš)
5. ë‹¤ìŒ ì£¼ ì£¼ìš” ê³„íš: ë²ˆí˜¸ ë§¤ê¸´ ë¦¬ìŠ¤íŠ¸ (êµ¬ì²´ì  íƒœìŠ¤í¬ 3~5ê°œ, ë‹´ë‹¹ìÂ·ì˜ˆì • ì™„ë£Œì¼ í¬í•¨)
6. í•µì‹¬ ë©”íŠ¸ë¦­ ìš”ì•½: bullet points ë˜ëŠ” ê°„ë‹¨ í…Œì´ë¸” (ì™„ë£Œìœ¨ XX%, ì§€ì—° íƒœìŠ¤í¬ ìˆ˜ Xê±´, ì˜¤í”ˆ ë¦¬ìŠ¤í¬ ìˆ˜ Xê±´ ë“±)"""

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì„ íƒ
        is_gemma = self.model_path and "gemma" in self.model_path.lower()
        is_qwen = self.model_path and "qwen" in self.model_path.lower()

        if is_gemma:
            # Gemma 3: <start_of_turn>user/model í˜•ì‹ (system role ì—†ìŒ)
            prompt_parts.append(f"<start_of_turn>user\n{system_prompt}<end_of_turn>")
            prompt_parts.append("<start_of_turn>model\në„¤, ì•Œê² ìŠµë‹ˆë‹¤.<end_of_turn>")

            for msg in context[-5:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "user":
                    prompt_parts.append(f"<start_of_turn>user\n{content}<end_of_turn>")
                elif role == "assistant":
                    prompt_parts.append(f"<start_of_turn>model\n{content}<end_of_turn>")

            if retrieved_docs and len(retrieved_docs) > 0:
                # RAG ë¬¸ì„œë¥¼ ê°„ê²°í•˜ê²Œ ì œê³µí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìœ ë„
                context_text = ""
                for doc in retrieved_docs[:3]:  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
                    doc_content = doc if isinstance(doc, str) else doc.get('content', str(doc))
                    # í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ (300ì ì œí•œ)
                    if len(doc_content) > 300:
                        doc_content = doc_content[:300]
                    context_text += doc_content + "\n"
                user_msg = f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n{context_text}\nì§ˆë¬¸: {message}"
                prompt_parts.append(f"<start_of_turn>user\n{user_msg}<end_of_turn>")
            else:
                prompt_parts.append(f"<start_of_turn>user\n{message}<end_of_turn>")

            prompt_parts.append("<start_of_turn>model\n")
        elif is_qwen:
            # Qwen3: ChatML í˜•ì‹ + /no_think ëª¨ë“œ (hallucination ìµœì†Œí™”)
            prompt_parts.append("<|im_start|>system")
            prompt_parts.append(system_prompt)
            prompt_parts.append("<|im_end|>")

            for msg in context[-5:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "user":
                    prompt_parts.append("<|im_start|>user")
                    prompt_parts.append(content)
                    prompt_parts.append("<|im_end|>")
                elif role == "assistant":
                    prompt_parts.append("<|im_start|>assistant")
                    prompt_parts.append(content)
                    prompt_parts.append("<|im_end|>")

            prompt_parts.append("<|im_start|>user")
            if retrieved_docs and len(retrieved_docs) > 0:
                # RAG documents: limit to 3 docs, 300 chars each
                context_text = ""
                for doc in retrieved_docs[:3]:
                    doc_content = doc if isinstance(doc, str) else doc.get('content', str(doc))
                    if len(doc_content) > 300:
                        doc_content = doc_content[:300]
                    context_text += doc_content + "\n"
                prompt_parts.append(f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n{context_text}\nì§ˆë¬¸: {message} /no_think")
            else:
                prompt_parts.append(f"{message} /no_think")
            prompt_parts.append("<|im_end|>")
            prompt_parts.append("<|im_start|>assistant")
        else:
            # ChatML: <|im_start|>system/user/assistant í˜•ì‹ (LFM2, Llama ë“±)
            prompt_parts.append("<|im_start|>system")
            prompt_parts.append(system_prompt)
            prompt_parts.append("<|im_end|>")

            for msg in context[-5:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "user":
                    prompt_parts.append("<|im_start|>user")
                    prompt_parts.append(content)
                    prompt_parts.append("<|im_end|>")
                elif role == "assistant":
                    prompt_parts.append("<|im_start|>assistant")
                    prompt_parts.append(content)
                    prompt_parts.append("<|im_end|>")

            prompt_parts.append("<|im_start|>user")
            if retrieved_docs and len(retrieved_docs) > 0:
                # RAG documents: limit to 3 docs, 300 chars each (same as Gemma)
                context_text = ""
                for doc in retrieved_docs[:3]:
                    doc_content = doc if isinstance(doc, str) else doc.get('content', str(doc))
                    if len(doc_content) > 300:
                        doc_content = doc_content[:300]
                    context_text += doc_content + "\n"
                prompt_parts.append(f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n{context_text}\nì§ˆë¬¸: {message}")
            else:
                prompt_parts.append(message)
            prompt_parts.append("<|im_end|>")
            prompt_parts.append("<|im_start|>assistant")

        return "\n".join(prompt_parts)

    def _clean_response(self, reply: str) -> str:
        """ì‘ë‹µ ì •ë¦¬"""

        # ëª¨ë¸ì´ ìê¸° ëŒ€í™”ë¥¼ ì‹œì‘í•˜ëŠ” íŒ¨í„´ì—ì„œ ì²« ì‘ë‹µë§Œ ì¶”ì¶œ
        for stop_pattern in ["ì§ˆë¬¸:", "\nmodel", "í•™ë…„\në°ìš”"]:
            if stop_pattern in reply:
                reply = reply.split(stop_pattern)[0].strip()

        # "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤" ì‹œì‘ íŒ¨í„´ ì œê±°
        if reply.startswith("ë„¤, ì•Œê² ìŠµë‹ˆë‹¤"):
            reply = reply[len("ë„¤, ì•Œê² ìŠµë‹ˆë‹¤"):].strip()
            if reply.startswith(".") or reply.startswith("ã€‚"):
                reply = reply[1:].strip()

        # Gemma íŠ¹ìˆ˜ í† í° ì œê±°
        reply = reply.replace("<start_of_turn>", "").replace("<end_of_turn>", "")
        # im_end í† í° ì œê±° (ê¹¨ì§€ëŠ” ë¬¸ì ë°©ì§€)
        reply = reply.replace("<|im_end|>", "").replace("|im_end|>", "").replace("<|im_end", "")

        # ì‚¼ì¤‘ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë¸”ë¡ ì œê±° (ëª¨ë¸ ì´ë¦„, êµ¬ë¶„ì„ , ì§ˆë¬¸ ë“± í¬í•¨)
        reply = re.sub(r"'''[\s\S]*?'''", "", reply)
        reply = re.sub(r'"""[\s\S]*?"""', "", reply)
        if reply.startswith("'''") or reply.startswith('"""'):
            reply = reply[3:].lstrip()
        if reply.endswith("'''") or reply.endswith('"""'):
            reply = reply[:-3].rstrip()
        
        # Qwen3 thinking í† í° ì œê±°
        reply = re.sub(r"<think>[\s\S]*?</think>", "", reply)

        # ëª¨ë¸ ì´ë¦„ê³¼ êµ¬ë¶„ì„ ì´ í¬í•¨ëœ ì•ë¶€ë¶„ ì œê±°
        # ì˜ˆ: "Llama Forge Model 2 (LFM2)\n===\nì§ˆë¬¸ë‚´ìš©"
        reply = re.sub(r"^.*?(Llama Forge Model|Gemma|LFM2|Qwen|ë¡œì»¬ LLM).*?\n=+\n.*?\n", "", reply, flags=re.MULTILINE | re.IGNORECASE)
        reply = re.sub(r"^.*?=+\n.*?\n", "", reply, flags=re.MULTILINE)
        
        # ë¶ˆí•„ìš”í•œ role ë ˆì´ë¸” ì œê±°
        if reply.startswith("model"):
            reply = reply[5:].strip()
        if reply.startswith("assistant"):
            reply = reply[9:].strip()

        # í”„ë¡¬í”„íŠ¸ í˜•ì‹ íƒœê·¸ ì œê±°
        reply = reply.replace("<think>", "")
        reply = reply.replace("system", "")
        reply = reply.replace("ì‚¬ìš©ì:", "")
        reply = reply.replace("user:", "")
        reply = reply.replace("USER", "")
        reply = reply.replace("_assistant", "")
        reply = reply.replace("assistant", "")
        
        # "í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”" ê°™ì€ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì œê±°
        unwanted_patterns = [
            "í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”",
            "í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”",
            "ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”",
            "ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”",
            "Please write an answer",
            "Write an answer",
            "ë‹µë³€ì€ 3~6ë¬¸ì¥",
            "í•µì‹¬ ì •ì˜",
            "ëª©ì /ë°°ê²½",
            "ê°„ë‹¨í•œ ì˜ˆì‹œ",
        ]
        
        # ë©”íƒ€ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±° (ë’¤ì— ë¶™ëŠ” ë¶ˆí•„ìš”í•œ ì„¤ëª…)
        meta_patterns = [
            r"ì œê³µëœ ì •ë³´ë¡œ.*?ì™„ë²½í•˜ê²Œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤.*?",
            r"ì œê³µëœ ì •ë³´ë¡œ.*?ë‹µë³€í–ˆìŠµë‹ˆë‹¤.*?",
            r"ì´ì œ ì‚¬ìš©ìë‹˜ì˜ ìš”ì²­ëŒ€ë¡œ.*?ì œê³µ",
            r"ì´ì œ ì‚¬ìš©ìì˜ ìš”ì²­ëŒ€ë¡œ.*?ì œê³µ",
            r"ì‚¬ìš©ìë‹˜ì˜ ìš”ì²­ëŒ€ë¡œ.*?ì„¤ëª….*?ì œê³µ",
            r"ì‚¬ìš©ìì˜ ìš”ì²­ëŒ€ë¡œ.*?ì„¤ëª….*?ì œê³µ",
            r"ìš”ì²­ëŒ€ë¡œ.*?í•œêµ­ì–´ë¡œ.*?ì œê³µ",
            r"ìš”ì²­í•˜ì‹ .*?í•œêµ­ì–´ë¡œ.*?ì œê³µ",
        ]
        for pattern in meta_patterns:
            reply = re.sub(pattern, "", reply, flags=re.IGNORECASE | re.DOTALL)
        
        # ì˜ëª»ëœ ëª¨ë¸ ì´ë¦„ í•„í„°ë§ (ëª¨ë¸ ì´ë¦„ ì§ˆë¬¸ì¸ ê²½ìš°)
        wrong_model_names = ["ë‹ˆì½œë¼ìŠ¤", "nicolas", "ì•Œë ‰ìŠ¤", "alex", "ì‚¬ë¼", "sara", 
                            "gpt-4", "chatgpt", "claude", "gemini", "palm", "gpt4"]
        
        # ì •í™•í•œ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        correct_name = "ë¡œì»¬ LLM"
        if self.model_path:
            import os
            model_file = os.path.basename(self.model_path)
            if "lfm2" in model_file.lower():
                correct_name = "Llama Forge Model 2 (LFM2)"
            elif "gemma" in model_file.lower():
                correct_name = "Gemma 3"
            elif "qwen" in model_file.lower():
                correct_name = "Qwen3-8B"
        
        # ì˜ëª»ëœ ëª¨ë¸ ì´ë¦„ì´ í¬í•¨ëœ ê²½ìš° ê°•ì œë¡œ êµì²´
        reply_lower = reply.lower()
        found_wrong_name = False
        for wrong_name in wrong_model_names:
            if wrong_name.lower() in reply_lower:
                found_wrong_name = True
                # ì •í™•í•œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì™„ì „íˆ êµì²´
                reply = f"ì €ëŠ” {correct_name} ëª¨ë¸ì…ë‹ˆë‹¤."
                break
        
        # Model name keyword detection - only trigger when asking about the model itself
        # Check for explicit model identity questions, not general use of "name" word
        # Keywords like "ì´ë¦„" can appear in normal context (e.g., "sprint's name"), so be more specific
        model_identity_patterns = [
            r"ë„ˆ(ëŠ”|ì˜)\s*(ì´ë¦„|ëª¨ë¸)",  # "ë„ˆëŠ” ì´ë¦„", "ë„ˆì˜ ëª¨ë¸"
            r"ë‹¹ì‹ (ì€|ì˜)\s*(ì´ë¦„|ëª¨ë¸)",  # "ë‹¹ì‹ ì€ ì´ë¦„", "ë‹¹ì‹ ì˜ ëª¨ë¸"
            r"(ë¬´ìŠ¨|ì–´ë–¤|ë­”)\s*ëª¨ë¸",  # "ë¬´ìŠ¨ ëª¨ë¸", "ì–´ë–¤ ëª¨ë¸"
            r"ëª¨ë¸\s*(ì´ë¦„|ëª…)",  # "ëª¨ë¸ ì´ë¦„", "ëª¨ë¸ëª…"
            r"(what|which)\s*model",  # English patterns
            r"your\s*name",
        ]
        import re as regex_module
        has_model_identity_question = any(regex_module.search(pattern, reply_lower) for pattern in model_identity_patterns)
        has_correct_name = any(correct in reply for correct in ["Llama", "Gemma", "Qwen", "ë¡œì»¬ LLM", "LFM2"])

        if has_model_identity_question and not has_correct_name:
            # This is a model identity question with wrong/missing answer - replace
            reply = f"ì €ëŠ” {correct_name} ëª¨ë¸ì…ë‹ˆë‹¤."
        for pattern in unwanted_patterns:
            reply = reply.replace(pattern, "")
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì œê±°
            reply = re.sub(re.escape(pattern), "", reply, flags=re.IGNORECASE)

        # assistant ì ‘ë‘ì–´ ì •ë¦¬
        cleaned_lines = []
        for line in reply.splitlines():
            stripped = line.strip()
            lower = stripped.lower()
            
            # ëª¨ë¸ ì´ë¦„ê³¼ êµ¬ë¶„ì„ ì´ í¬í•¨ëœ ì¤„ ì œê±°
            if re.search(r"(llama forge model|gemma|lfm2|ë¡œì»¬ llm).*?===", lower) or re.search(r"^=+$", stripped):
                stripped = ""
            # ì‚¼ì¤‘ ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ê±°ë‚˜ ëë‚˜ëŠ” ì¤„ ì œê±°
            elif stripped.startswith("'''") or stripped.endswith("'''") or stripped.startswith('"""') or stripped.endswith('"""'):
                stripped = ""
            # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
            elif lower.startswith("assistant:") or lower.startswith("assistantï¼š"):
                stripped = stripped.split(":", 1)[1].strip() if ":" in stripped else ""
            elif lower == "assistant" or lower == "system" or lower == "user":
                stripped = ""
            elif stripped.startswith("ì‚¬ìš©ì:") or stripped.startswith("ì‚¬ìš©ìï¼š"):
                stripped = ""
            elif stripped.startswith("system") or stripped.startswith("user"):
                stripped = ""
            elif "<think>" in stripped.lower():
                stripped = ""
            elif any(pattern in stripped for pattern in unwanted_patterns):
                stripped = ""
            # ë©”íƒ€ ì„¤ëª… í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì¤„ ì œê±°
            elif re.search(r"ì œê³µëœ ì •ë³´ë¡œ.*?ë‹µë³€í–ˆìŠµë‹ˆë‹¤", lower) or re.search(r"ì´ì œ ì‚¬ìš©ì.*?ìš”ì²­ëŒ€ë¡œ", lower) or re.search(r"ìš”ì²­.*?í•œêµ­ì–´ë¡œ.*?ì œê³µ", lower):
                stripped = ""
            
            if stripped:
                cleaned_lines.append(stripped)
        
        if cleaned_lines:
            reply = "\n".join(cleaned_lines)
        else:
            # ëª¨ë“  ì¤„ì´ ì œê±°ëœ ê²½ìš° ì›ë³¸ì—ì„œ ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„ë§Œ ì‚¬ìš©
            lines = reply.splitlines()
            for line in lines:
                stripped = line.strip()
                if stripped and not any(unwanted in stripped.lower() for unwanted in ["system", "user", "assistant", "ì‚¬ìš©ì", "<redacted"]):
                    reply = stripped
                    break

        # ì‘ë‹µ ì•ë¶€ë¶„ì—ì„œ ëª¨ë¸ ì´ë¦„ê³¼ êµ¬ë¶„ì„  ì œê±°
        # ì˜ˆ: "Llama Forge Model 2 (LFM2)\n===\nì§ˆë¬¸ë‚´ìš©\n\në‹µë³€ë‚´ìš©" -> "ë‹µë³€ë‚´ìš©"
        lines = reply.splitlines()
        start_idx = 0
        for i, line in enumerate(lines):
            stripped = line.strip()
            # ëª¨ë¸ ì´ë¦„ì´ë‚˜ êµ¬ë¶„ì„ ì´ ìˆëŠ” ì¤„ì€ ê±´ë„ˆë›°ê¸°
            if re.search(r"(llama forge model|gemma|lfm2|ë¡œì»¬ llm)", stripped, re.IGNORECASE) or re.match(r"^=+$", stripped):
                start_idx = i + 1
            # ì‚¬ìš©ì ì§ˆë¬¸ì²˜ëŸ¼ ë³´ì´ëŠ” ì¤„ë„ ê±´ë„ˆë›°ê¸° (ì§ˆë¬¸ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°)
            elif (stripped.endswith("?") or stripped.endswith("ì£¼ì„¸ìš”") or stripped.endswith("í•´ì£¼ì„¸ìš”")) and i < len(lines) - 1:
                start_idx = i + 1
            else:
                break
        
        if start_idx > 0:
            reply = "\n".join(lines[start_idx:]).strip()
        
        # ì‘ë‹µ ë’·ë¶€ë¶„ì—ì„œ ë©”íƒ€ ì„¤ëª… ì œê±°
        lines = reply.splitlines()
        end_idx = len(lines)
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            # ë©”íƒ€ ì„¤ëª… íŒ¨í„´ì´ ìˆìœ¼ë©´ ê·¸ ì¤„ë¶€í„° ëê¹Œì§€ ì œê±°
            if re.search(r"ì œê³µëœ ì •ë³´ë¡œ.*?ë‹µë³€í–ˆìŠµë‹ˆë‹¤", line, re.IGNORECASE) or \
               re.search(r"ì´ì œ ì‚¬ìš©ì.*?ìš”ì²­ëŒ€ë¡œ", line, re.IGNORECASE) or \
               re.search(r"ìš”ì²­.*?í•œêµ­ì–´ë¡œ.*?ì œê³µ", line, re.IGNORECASE) or \
               re.search(r"ì™„ë²½í•˜ê²Œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤", line, re.IGNORECASE):
                end_idx = i
                break
        
        if end_idx < len(lines):
            reply = "\n".join(lines[:end_idx]).strip()

        # ì¤‘ë³µ ì‘ë‹µ ë°©ì§€
        if "<start_of_turn>" in reply:
            reply = reply.split("<start_of_turn>")[0].strip()
        
        # im_end í† í°ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if "<|im_end|>" in reply:
            reply = reply.split("<|im_end|>")[0].strip()
        if "|im_end|>" in reply:
            reply = reply.split("|im_end|>")[0].strip()

        # ê³¼ë„í•˜ê²Œ ê¸´ ì‘ë‹µ ì œí•œ
        if "\n\n\n" in reply:
            reply = reply.split("\n\n\n")[0].strip()

        # ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±° (ê°™ì€ ë¬¸ë‹¨ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ëŠ” ê²½ìš°)
        lines = reply.split('\n')
        seen_lines = set()
        unique_lines = []
        for line in lines:
            line_stripped = line.strip()
            if line_stripped and line_stripped not in seen_lines:
                seen_lines.add(line_stripped)
                unique_lines.append(line)
            elif not line_stripped:  # ë¹ˆ ì¤„ì€ ìœ ì§€
                unique_lines.append(line)
        reply = '\n'.join(unique_lines)
        
        # ì œì–´ ë¬¸ì ë° ê¹¨ì§€ëŠ” ë¬¸ì ì œê±° (ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€)
        import string
        # ì¸ì‡„ ê°€ëŠ¥í•œ ë¬¸ìì™€ ê³µë°±ë§Œ ìœ ì§€
        printable_chars = set(string.printable)
        # í•œê¸€, í•œì, ì¼ë³¸ì–´ ë“± ìœ ë‹ˆì½”ë“œ ë¬¸ìë„ í—ˆìš©
        cleaned_chars = []
        for char in reply:
            # ì¸ì‡„ ê°€ëŠ¥í•œ ASCII ë¬¸ìì´ê±°ë‚˜, ìœ ë‹ˆì½”ë“œ ë¬¸ì(í•œê¸€ ë“±)ì¸ ê²½ìš°ë§Œ ìœ ì§€
            if char in printable_chars or ord(char) > 127:
                # ì œì–´ ë¬¸ì ì œê±° (íƒ­, ì¤„ë°”ê¿ˆ, ìºë¦¬ì§€ ë¦¬í„´ì€ ìœ ì§€)
                if ord(char) < 32 and char not in ['\n', '\r', '\t']:
                    continue
                cleaned_chars.append(char)
        reply = ''.join(cleaned_chars)
        
        # ì•ë’¤ ê³µë°± ì •ë¦¬
        reply = reply.strip()
        
        # ì‘ë‹µ ëì— ë‚¨ì€ ë¶ˆì™„ì „í•œ íƒœê·¸ë‚˜ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        # ì˜ˆ: "<", "<start", "<end", "<|" ë“±
        while reply and reply[-1] in ['<', '>', '|']:
            reply = reply[:-1].strip()
        
        # ë¶ˆì™„ì „í•œ íƒœê·¸ íŒ¨í„´ ì œê±° (ëë¶€ë¶„ì— ë‚¨ì€ ê²ƒë“¤)
        reply = re.sub(r'<[^>]*$', '', reply)  # ëì— ë¶ˆì™„ì „í•œ íƒœê·¸ ì œê±°
        reply = re.sub(r'\|[^>]*$', '', reply)  # ëì— ë¶ˆì™„ì „í•œ í† í° ì œê±°
        
        # ë‹¤ì‹œ ì•ë’¤ ê³µë°± ì •ë¦¬
        reply = reply.strip()

        # ì‘ë‹µ ì‹œì‘ ë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ë¶€í˜¸ ì œê±° (., ã€‚, :, ï¼š, -, ë“±)
        while reply and reply[0] in '.ã€‚:ï¼š-â€“â€”â€¢Â·':
            reply = reply[1:].strip()

        return reply

    def _calculate_confidence(self, intent: str, retrieved_docs: List[str]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""

        base_confidence = {
            "casual": 0.95,      # ì¼ìƒ ëŒ€í™”ëŠ” ë†’ì€ ì‹ ë¢°ë„
            "pms_query": 0.70,   # PMS ì§ˆë¬¸ì€ RAG ì˜ì¡´
            "general": 0.80      # ì¼ë°˜ ì§ˆë¬¸ì€ ì¤‘ê°„
        }.get(intent, 0.75)

        # RAG ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if retrieved_docs and len(retrieved_docs) > 0:
            rag_boost = min(0.15, len(retrieved_docs) * 0.05)
            base_confidence = min(0.95, base_confidence + rag_boost)

        return round(base_confidence, 2)

    def run(self, message: str, context: List[dict] = None, retrieved_docs: List[str] = None) -> dict:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""

        initial_state: ChatState = {
            "message": message,
            "context": context or [],
            "intent": None,
            "retrieved_docs": retrieved_docs or [],
            "response": None,
            "confidence": 0.0,
            "debug_info": {},

            # ì¿¼ë¦¬ ê°œì„  ê´€ë ¨ í•„ë“œ ì´ˆê¸°í™”
            "current_query": message,
            "retry_count": 0,
            "extracted_terms": []
        }

        logger.info(f"Starting workflow for message: {message[:50]}...")

        # ê·¸ë˜í”„ ì‹¤í–‰
        final_state = self.graph.invoke(initial_state)

        logger.info(f"Workflow completed. Intent: {final_state.get('intent')}, "
                   f"RAG docs: {len(final_state.get('retrieved_docs', []))}, "
                   f"Retries: {final_state.get('retry_count', 0)}")

        # ë””ë²„ê·¸ ì •ë³´ì— ì¿¼ë¦¬ ê°œì„  ì •ë³´ ì¶”ê°€
        debug_info = final_state.get("debug_info", {})
        if final_state.get("retry_count", 0) > 0:
            debug_info["query_refinement"] = {
                "original_query": message,
                "final_query": final_state.get("current_query", message),
                "retry_count": final_state.get("retry_count", 0),
                "extracted_terms": final_state.get("extracted_terms", [])
            }

        return {
            "reply": final_state.get("response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
            "confidence": final_state.get("confidence", 0.0),
            "intent": final_state.get("intent"),
            "rag_docs_count": len(final_state.get("retrieved_docs", [])),
            "debug_info": debug_info
        }
