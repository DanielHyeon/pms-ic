# ============================================
# Database Configuration
# ============================================
POSTGRES_DB=pms_db
POSTGRES_USER=pms_user
POSTGRES_PASSWORD=change-me-in-production

# ============================================
# Backend Configuration
# ============================================
SPRING_PROFILES_ACTIVE=dev
JWT_SECRET=change-me-in-production-min-256-bits-required

# ============================================
# AI Service Configuration
# ============================================
AI_TEAM_API_URL=http://llm-service:8000
AI_TEAM_MODEL=google.gemma-3-12b-pt.Q5_K_M.gguf

# ============================================
# Redis Configuration
# ============================================
REDIS_HOST=redis
REDIS_PORT=6379

# ============================================
# Neo4j Configuration (GraphRAG)
# ============================================
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=change-me-in-production

# ============================================
# Frontend Configuration
# ============================================
VITE_API_URL=http://localhost:8080/api

# ============================================
# Development Tools
# ============================================
PGADMIN_DEFAULT_EMAIL=admin@pms.com
PGADMIN_DEFAULT_PASSWORD=admin

# ============================================
# LLM Service Configuration (GPU/CPU Mode)
# ============================================
# Set LLM_MODE to either "cpu" or "gpu"
# Default: cpu (CPU-only inference)
LLM_MODE=cpu

# LLM Dockerfile selection:
# - Dockerfile.cpu: CPU-only optimized
# - Dockerfile.gpu: GPU-optimized (requires NVIDIA Container Toolkit)
LLM_DOCKERFILE=Dockerfile.cpu

# When LLM_MODE=cpu:
LLM_N_GPU_LAYERS=0
LLM_N_CTX=2048
LLM_N_THREADS=8
EMBEDDING_DEVICE=cpu

# When LLM_MODE=gpu (change these values):
# LLM_N_GPU_LAYERS=50
# LLM_N_CTX=4096
# LLM_N_THREADS=6
# EMBEDDING_DEVICE=cuda

# ============================================
# Mail Configuration
# ============================================
MAIL_HOST=localhost
MAIL_PORT=1025
MAIL_USERNAME=
MAIL_PASSWORD=

# ============================================
# Two-Track LLM Configuration (L1/L2)
# ============================================
# L1: Fast model for high-frequency queries (Track A)
# L2: Quality model for high-value outputs (Track B)
L1_MODEL_PATH=./models/LFM2-2.6B-Uncensored-X64.i1-Q6_K.gguf
L2_MODEL_PATH=./models/google.gemma-3-12b-pt.Q5_K_M.gguf
L1_N_CTX=4096
L2_N_CTX=4096
L1_MAX_TOKENS=1200
L2_MAX_TOKENS=3000

# ============================================
# PG-Neo4j Sync Configuration
# ============================================
# Enable automatic sync from PostgreSQL to Neo4j
SYNC_ENABLED=true
SYNC_FULL_INTERVAL_HOURS=24
SYNC_INCREMENTAL_INTERVAL_MINUTES=5

# ============================================
# vLLM Worker Configuration (Production LLM)
# ============================================
# Enable vLLM for production workloads
VLLM_ENABLED=false
VLLM_WORKER_URL=http://vllm-service:8000
VLLM_PORT=8001

# Model Configuration
VLLM_MODEL=Qwen/Qwen2.5-7B-Instruct
VLLM_SERVED_MODEL_NAME=default

# GPU Configuration
VLLM_GPU_COUNT=1
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_MAX_MODEL_LEN=8192
VLLM_DTYPE=auto

# Model Storage Path
VLLM_MODEL_PATH=./models/vllm

# Hugging Face Token (required for gated models like Llama)
HF_TOKEN=

# Tool Calling Configuration
VLLM_ENABLE_AUTO_TOOL_CHOICE=true
VLLM_TOOL_CALL_PARSER=hermes

# ============================================
# GGUF Worker Configuration (Development LLM)
# ============================================
GGUF_ENABLED=true
GGUF_WORKER_URL=http://llm-service:8000

# ============================================
# LLM Gateway Configuration
# ============================================
# Default engine: auto, gguf, vllm, ab
LLM_DEFAULT_ENGINE=auto

# Timeout Configuration (seconds)
LLM_TIMEOUT_TOTAL=120
LLM_TIMEOUT_TTFT=15
LLM_TIMEOUT_REPORT=300

# Concurrency Limits
LLM_CONCURRENT_GGUF=2
LLM_CONCURRENT_VLLM=10

# Routing Configuration
LLM_CONTEXT_THRESHOLD=4096
LLM_TOOLS_PREFER_VLLM=true

# ============================================
# Production Configuration (override in production)
# ============================================
# JWT_SECRET=<generate-secure-random-256bit-key>
# POSTGRES_PASSWORD=<secure-password>
# NEO4J_PASSWORD=<secure-password>
